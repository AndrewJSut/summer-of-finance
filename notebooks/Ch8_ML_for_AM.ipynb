{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML for Asset Management - Marcos Lopez de Prado \n",
    "\n",
    "**Date:** 2025-06-03\n",
    "\n",
    "### Section 8 - Test Set Overfitting\n",
    "\n",
    "Monte Carlo simulations play in mathematics the analogue to a controlled experiment in the physical sciences.\n",
    "\n",
    "A backtest is a historical simulation of how an investment strategy would have performed in the past. It is not a controlled experiment, because we cannot change the environmental variables to derive a new historical time series on which to perform an independent backtest.\n",
    "\n",
    "It is easy for a researcher to overfit a backtest, by conducting multiple historical simulations, and selecting the best performing strategy.\n",
    "\n",
    "When a researcher presents an overfit backtest as the outcome of a single trial, the simulated performance is inflated. This form of statistical inflation is called selection bias under multiple testing (SBuMT)\n",
    "\n",
    "SBuMT is compounded by asset managers through sequential SBuMT at two levels: \n",
    "- (1) each researcher runs millions of simulations and presents the best (overfit) ones to her boss; \n",
    "- (2) the company further selects a few backtests among the (already overfit) backtests submitted by the researchers. \n",
    "\n",
    "We may call this backtest hyperfitting, to differentiate it from backtest overfitting.\n",
    "\n",
    "In this section, we study how researchers can estimate the effect that SBuMT has on their findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Precision and Recall\n",
    "\n",
    "Consider $s$ investment strategies. Some of these are false discoveries, in the sense that their expected return is not positive. \n",
    "\n",
    "We can decompose these strategies into true and false: $s = s_T+s_F$. Let $\\theta$ be the odds ratio of true strategies against false strategies: $\\theta = \\frac{s_T}{s_F}$.\n",
    "\n",
    "In financial economics, $\\theta$ is expected to be low. The number of true investment strategies is: $$s_T = s \\frac{\\theta}{1+\\theta}$$ and the number of false investment strategies is: $$s_F = s \\frac{1}{1+\\theta}$$\n",
    "\n",
    "Give a false positive rate $\\alpha$, we will obtain a number of false positives $FP = \\alpha s_F$ and a number of true negatives $TN = (1-\\alpha)s_F$. Let us denote by $\\beta$ the false negative rate associated with $\\alpha$. We will obtain a nmber of false negatives $FN = \\beta s_T$ and true positives $TP = (1-\\beta) s_T$. Therefore the precision and recall of our test are: $$Precision = \\frac{TP}{TP+FP} = \\frac{(1-\\beta)\\theta}{(1-\\beta)\\theta+\\alpha}$$\n",
    "\n",
    "$$recall = \\frac{TP}{TP+FN}=1-\\beta$$\n",
    "\n",
    "Before running backtests on a strategy, researchers should gather evidence that a strategy may indeed exist. The reason is the precision of the test is a function of the odds ratio, \\theta. If the odds ratio is love, the precision will be low, even if we get a positive with high confidence (low $p$-value).\n",
    "\n",
    "For example: Suppose that the probability of a backtested strategy being profitable is 0.01, that is, that one out of one hundred strategies is true, hence $\\theta$=1/99. Then, at the standard thresholds of $\\alpha = 0.05, \\beta = 0.2$, researchers are expected to get approximately fifty-eight positives out one thousand trials, where approximately eight are true positives, and approximately fifty are false positives. Under these circumstances, a p-value of 0.05 implies a false discovery rate of 86.09% (roughly 50/58). For this reason alone, we should expect that most discoveries in financial econometrics are likely false."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3 Precision and Recall under multiple testing\n",
    "\n",
    "After one trial, the probability of making a type I error is $\\alpha$, and the probability of not making a type I error is $1-\\alpha$. IF two trials are independant, the probability of not making a type I error on the first *and* second tests is $(1-\\alpha)^2$. If we conduct K independent trials, the joint probability of not making a single type I error is $(1-\\alpha)^K$. Hence the probability of making at least one type I error is the complement: $\\alpha_K = 1-(1-\\alpha)^K$. This is known as the familywise error rate (FWER).\n",
    "\n",
    "After one trial, the probability of making a type II error is $\\beta$. After K independent trials, the probability of making a type II error on them is $\\beta_K = \\beta^K$. \n",
    "\n",
    "Note the difference with FWER. In the false positive case, we are interested in the probability of making at least one error. This is because a single false alarm is a failure. However, in the false negative case, we are interested in the probability that all positives are missed. As K increases, $\\alpha_K$ grows and $\\beta_K$ shrinks.\n",
    "\n",
    "Precision and recall adjusted for multiple testing are:\n",
    "\n",
    "$$Precision = \\frac{(1-\\beta_K)\\theta}{(1-\\beta_K)\\theta+\\alpha_K}=\\frac{(1-\\beta_K)\\theta}{(1-\\beta_K)\\theta+1-(1-\\alpha)^K}$$\n",
    "\n",
    "$$recall = \\frac{TP}{TP+FN}=1-\\beta_K=1-\\beta^K$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4 The Sharpe Ratio\n",
    "\n",
    "Financial analysts do not typically assess the performance of a strategy in terms of precision and recall. The most common measure of strategy performance is the Sharpe ratio. \n",
    "\n",
    "In what follows, we will develop a framework for assessing the probability that a strategy is false. The inputs are the Sharpe ratio estimate, as well as metadata captured during the discovery process.\n",
    "\n",
    "Consider an investment strategy with excess returns (or risk premia) $\\{r_t\\}$, $t=1,...,T$, which are independent and IID normal, $r_t \\sim N[\\mu,\\sigma^2]$. The nonannualized Sharpe ratio of such a strategy is defined as $$SR=\\frac{\\mu}{\\sigma}$$\n",
    "\n",
    "Because these parameters are not known, SR is esitmated as:\n",
    "\n",
    "$$\\hat{SR}=\\frac{E[\\{r_t\\}]}{\\sqrt{V[\\{r_t\\}]}}$$\n",
    "\n",
    "Under the assumption that returns are IID normal, we can derive the asymptotic distribution of $\\hat{SR}$ as \n",
    "\n",
    "$$(\\hat{SR}-SR)\\rightarrow^a N [0,\\frac{1+\\frac{1}{2}SR^2}{T}]$$\n",
    "\n",
    "However, empirical evidence shows that hedge fund returns exhibit substantial negative skewness and positive excess kurtosis. Wrongly assuming the returns are IID Normal can lead to a gross underestimation of the false positive probability. \n",
    "\n",
    "Under the assumption that returns are drawn from IID non-Normal distributions, Mertons derived the asymptotic distribution as $$(\\hat{SR}-SR)\\rightarrow^a N [0,\\frac{1+\\frac{1}{2}SR^2-\\gamma_3SR+\\frac{\\gamma_4-3}{4}SR^2}{T}]$$ where $\\gamma_3$ is the skewness of $\\{r_t\\}$ and $\\gamma_4$ is the kurtosis of $\\{r_t\\}$. Shortly after, researchers discovered that this equation is also valid under the more general assumption that returns are stationary and ergodic. \n",
    "\n",
    "The key implication is that $\\hat{SR}$ still follows a Normal distribution even if returns are non-Normal, however with a variance that partly depends on the skewness and kurtosis of the returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The \"False Strategy Theorem\"\n",
    "\n",
    "A researcher may carry out a large number of historical simulations (trials), and report only the best outcome (maximum Sharpe ratio).\n",
    "\n",
    "The distribution of the maximum Sharpe ratio is not the same as the distribution of a Sharpe ratio randomly chosen among the trials, hence giving rise to SBuMT.\n",
    "\n",
    "When more than one trial takes place, the expected value of the maximum Sharpe ratio is greater than the expected value of the Sharpe ratio from a random trial.\n",
    "\n",
    "In particular, given an investment strategy with expected Sharpe ratio zero and nonnull variance, the expected value of the maximum Sharpe ratio is strictly positive, and a function of the number of trials.\n",
    "\n",
    "Given the above, the magnitude of SBuMT can be expressed in terms of the difference between the expected maximum Sharpe ratio and the expected Sharpe ratio from a random trial (zero, in the case of a false strategy). As it turns out, SBuMT is a function of two variables: the number of trials, and the variance of the Sharpe ratios across trials. The following theorem formally states that relationship. \n",
    "\n",
    "Theorem: Given a sample of estimated performance statistics $\\{\\hat{SR}_k\\}$, $k=1,...,K$, drawn from IID Gaussians, $\\hat{SR}_k \\sim N[0, V[\\hat{SR}_k]]$, then\n",
    "\n",
    "$$E[\\max_k \\{\\hat{SR}_k\\}](V[\\hat{SR}_k])^{-1/2} \\approx (1-\\gamma)Z^{-1}[1-\\frac{1}{K}]+\\gamma Z^{-1}[1-\\frac{1}{Ke}]$$\n",
    "\n",
    "Where $\\gamma$ is the Euler–Mascheroni constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.6 Experimental Results\n",
    "\n",
    "The False Strategy theorem provides us with an approximation of the expected maximum Sharpe ratio.\n",
    "\n",
    "An experimental analysis of this theorem can be useful at two levels. First, it can help us find evidence that the theorem is not true, and in fact the proof is flawed. Of course, the converse is not true: experimental evidence can never replace the role of a mathematical proof. Still, experimental evidence can point to problems with the proof, and give us a better understanding of what the proof should look like. \n",
    "\n",
    "Second, the theorem does not provide a boundary for the approximation. An experimental analysis can help us estimate the distribution of the approximation error.\n",
    "\n",
    "The following Monte Carlo experiment evaluates the accuracy of the False Strategy theorem:\n",
    "\n",
    "- First, given a pair of values $(K, V[\\hat{SR}_k])$, we generate a random array of size SxK, where S is the number of Monte Carlo experiments. The values contained by this random array are drawn from a Standard Normal distribution.\n",
    "\n",
    "- Second, the rows in this array are centered and scaled to match zero mean and $V[\\hat{SR}_k]$ variance. \n",
    "\n",
    "- Third, the maximum value accross each row is computed, resulting in S of such maxima. \n",
    "\n",
    "- Fourth, we compute the average value across S maxima, $\\hat{E}[\\max_k \\{\\hat{SR}_k\\}]$.\n",
    "\n",
    "- Fifth, this empirical estimate of the expected maximum SR can be compared with the analytical solution provided by the False Strategy theorem, $E[\\max_k \\{\\hat{SR}_k\\}]$.\n",
    "\n",
    "- Sixth, the estimation error is defined in relative terms to the predicted value as, $$\\epsilon = \\frac{\\hat{E}[\\max_k \\{\\hat{SR}_k\\}]}{E[\\max_k \\{\\hat{SR}_k\\}]}-1$$\n",
    "\n",
    "- Seventh, we repeat these steps R times, resulting in $\\{\\epsilon_r \\}_{r=1,...,R}$, allowing us to compute the mean and standard deviation of the esitmation errors associated with K trials. The following code implements this Monte Carlo experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing theoretical SR: 100%|██████████| 889/889 [00:00<00:00, 15661.56it/s]\n",
      "Simulating Sharpe Ratios:  86%|████████▌ | 766/889 [15:02<02:24,  1.18s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     51\u001b[39m sr_theoretical = pd.Series(\n\u001b[32m     52\u001b[39m     {i: getExpectedMaxSR(i, meanSR=meanSR, stdSR=stdSR) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(nTrials, desc=\u001b[33m\"\u001b[39m\u001b[33mComputing theoretical SR\u001b[39m\u001b[33m\"\u001b[39m)}\n\u001b[32m     53\u001b[39m )\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Empirical distribution of max SRs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m sr_empirical = \u001b[43mgetDistMaxSR\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnSims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnSims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnTrials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnTrials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeanSR\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmeanSR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdSR\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstdSR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Group empirical data to compute mean per nTrials\u001b[39;00m\n\u001b[32m     59\u001b[39m sr_empirical_mean = sr_empirical.groupby(\u001b[33m\"\u001b[39m\u001b[33mnTrials\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[33m\"\u001b[39m\u001b[33mmax_SR\u001b[39m\u001b[33m\"\u001b[39m].mean()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mgetDistMaxSR\u001b[39m\u001b[34m(nSims, nTrials, stdSR, meanSR)\u001b[39m\n\u001b[32m     29\u001b[39m sr = pd.DataFrame(rng.randn(\u001b[38;5;28mint\u001b[39m(nSims), nTrials_))\n\u001b[32m     30\u001b[39m sr = sr.sub(sr.mean(axis=\u001b[32m1\u001b[39m), axis=\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# center each row\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m sr = sr.div(\u001b[43msr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, axis=\u001b[32m0\u001b[39m)   \u001b[38;5;66;03m# standardize each row\u001b[39;00m\n\u001b[32m     32\u001b[39m sr = meanSR + sr * stdSR              \u001b[38;5;66;03m# scale to desired mean/std\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Store the max SR per simulation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Summer of Finance/summer-of-finance/venv/lib/python3.12/site-packages/pandas/core/frame.py:11748\u001b[39m, in \u001b[36mDataFrame.std\u001b[39m\u001b[34m(self, axis, skipna, ddof, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  11739\u001b[39m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[33m\"\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m\"\u001b[39m, ndim=\u001b[32m2\u001b[39m))\n\u001b[32m  11740\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstd\u001b[39m(\n\u001b[32m  11741\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  11746\u001b[39m     **kwargs,\n\u001b[32m  11747\u001b[39m ):\n\u001b[32m> \u001b[39m\u001b[32m11748\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  11749\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series):\n\u001b[32m  11750\u001b[39m         result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Summer of Finance/summer-of-finance/venv/lib/python3.12/site-packages/pandas/core/generic.py:12358\u001b[39m, in \u001b[36mNDFrame.std\u001b[39m\u001b[34m(self, axis, skipna, ddof, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12350\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstd\u001b[39m(\n\u001b[32m  12351\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  12352\u001b[39m     axis: Axis | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  12356\u001b[39m     **kwargs,\n\u001b[32m  12357\u001b[39m ) -> Series | \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m> \u001b[39m\u001b[32m12358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stat_function_ddof\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12359\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstd\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m  12360\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Summer of Finance/summer-of-finance/venv/lib/python3.12/site-packages/pandas/core/generic.py:12322\u001b[39m, in \u001b[36mNDFrame._stat_function_ddof\u001b[39m\u001b[34m(self, name, func, axis, skipna, ddof, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12319\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m  12320\u001b[39m     axis = \u001b[32m0\u001b[39m\n\u001b[32m> \u001b[39m\u001b[32m12322\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[43m=\u001b[49m\u001b[43mddof\u001b[49m\n\u001b[32m  12324\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Summer of Finance/summer-of-finance/venv/lib/python3.12/site-packages/pandas/core/frame.py:11562\u001b[39m, in \u001b[36mDataFrame._reduce\u001b[39m\u001b[34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[39m\n\u001b[32m  11558\u001b[39m     df = df.T\n\u001b[32m  11560\u001b[39m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[32m  11561\u001b[39m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[32m> \u001b[39m\u001b[32m11562\u001b[39m res = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  11563\u001b[39m out = df._constructor_from_mgr(res, axes=res.axes).iloc[\u001b[32m0\u001b[39m]\n\u001b[32m  11564\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out.dtype != \u001b[33m\"\u001b[39m\u001b[33mboolean\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Summer of Finance/summer-of-finance/venv/lib/python3.12/site-packages/pandas/core/internals/managers.py:1500\u001b[39m, in \u001b[36mBlockManager.reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m   1498\u001b[39m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] = []\n\u001b[32m   1499\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m-> \u001b[39m\u001b[32m1500\u001b[39m     nbs = \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1501\u001b[39m     res_blocks.extend(nbs)\n\u001b[32m   1503\u001b[39m index = Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Summer of Finance/summer-of-finance/venv/lib/python3.12/site-packages/pandas/core/internals/blocks.py:404\u001b[39m, in \u001b[36mBlock.reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) -> \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[32m    400\u001b[39m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.values.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    407\u001b[39m         res_values = result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Summer of Finance/summer-of-finance/venv/lib/python3.12/site-packages/pandas/core/frame.py:11481\u001b[39m, in \u001b[36mDataFrame._reduce.<locals>.blk_func\u001b[39m\u001b[34m(values, axis)\u001b[39m\n\u001b[32m  11479\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.array([result])\n\u001b[32m  11480\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m> \u001b[39m\u001b[32m11481\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Summer of Finance/summer-of-finance/venv/lib/python3.12/site-packages/pandas/core/nanops.py:147\u001b[39m, in \u001b[36mbottleneck_switch.__call__.<locals>.f\u001b[39m\u001b[34m(values, axis, skipna, **kwds)\u001b[39m\n\u001b[32m    145\u001b[39m         result = alt(values, axis=axis, skipna=skipna, **kwds)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Summer of Finance/summer-of-finance/venv/lib/python3.12/site-packages/pandas/core/nanops.py:950\u001b[39m, in \u001b[36mnanstd\u001b[39m\u001b[34m(values, axis, skipna, ddof, mask)\u001b[39m\n\u001b[32m    947\u001b[39m orig_dtype = values.dtype\n\u001b[32m    948\u001b[39m values, mask = _get_values(values, skipna, mask=mask)\n\u001b[32m--> \u001b[39m\u001b[32m950\u001b[39m result = np.sqrt(\u001b[43mnanvar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[43m=\u001b[49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_results(result, orig_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Summer of Finance/summer-of-finance/venv/lib/python3.12/site-packages/pandas/core/nanops.py:85\u001b[39m, in \u001b[36mdisallow.__call__.<locals>._f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m     82\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreduction operation \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not allowed for this dtype\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     83\u001b[39m     )\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# we want to transform an object array\u001b[39;00m\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[32m     90\u001b[39m     \u001b[38;5;66;03m# object arrays that contain strings\u001b[39;00m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(args[\u001b[32m0\u001b[39m]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Summer of Finance/summer-of-finance/venv/lib/python3.12/site-packages/pandas/core/nanops.py:147\u001b[39m, in \u001b[36mbottleneck_switch.__call__.<locals>.f\u001b[39m\u001b[34m(values, axis, skipna, **kwds)\u001b[39m\n\u001b[32m    145\u001b[39m         result = alt(values, axis=axis, skipna=skipna, **kwds)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Summer of Finance/summer-of-finance/venv/lib/python3.12/site-packages/pandas/core/nanops.py:1018\u001b[39m, in \u001b[36mnanvar\u001b[39m\u001b[34m(values, axis, skipna, ddof, mask)\u001b[39m\n\u001b[32m   1016\u001b[39m sqr = _ensure_numeric((avg - values) ** \u001b[32m2\u001b[39m)\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1018\u001b[39m     \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mputmask\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1019\u001b[39m result = sqr.sum(axis=axis, dtype=np.float64) / d\n\u001b[32m   1021\u001b[39m \u001b[38;5;66;03m# Return variance as np.float64 (the datatype used in the accumulator),\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[38;5;66;03m# unless we were dealing with a float array, in which case use the same\u001b[39;00m\n\u001b[32m   1023\u001b[39m \u001b[38;5;66;03m# precision as the original values array.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------------------------------------\n",
    "def getExpectedMaxSR(nTrials, meanSR, stdSR):\n",
    "    \"\"\"\n",
    "    Computes the expected maximum Sharpe Ratio (SR) from `nTrials`,\n",
    "    correcting for selection bias under multiple testing.\n",
    "    \"\"\"\n",
    "    emc = 0.5772156649015329  # Euler–Mascheroni constant\n",
    "    sr0 = (1 - emc) * norm.ppf(1 - 1. / nTrials) + emc * norm.ppf(1 - (nTrials * np.e) ** -1)\n",
    "    sr0 = meanSR + stdSR * sr0\n",
    "    return sr0\n",
    "\n",
    "# ---------------------------------------------------\n",
    "def getDistMaxSR(nSims, nTrials, stdSR, meanSR):\n",
    "    \"\"\"\n",
    "    Simulates the distribution of the maximum Sharpe Ratio across `nTrials`,\n",
    "    repeated `nSims` times. Returns a DataFrame of empirical max SRs.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(42)\n",
    "    out = pd.DataFrame()\n",
    "\n",
    "    for nTrials_ in tqdm(nTrials, desc=\"Simulating Sharpe Ratios\"):\n",
    "        # Simulate standard normal SRs\n",
    "        sr = pd.DataFrame(rng.randn(int(nSims), nTrials_))\n",
    "        sr = sr.sub(sr.mean(axis=1), axis=0)  # center each row\n",
    "        sr = sr.div(sr.std(axis=1), axis=0)   # standardize each row\n",
    "        sr = meanSR + sr * stdSR              # scale to desired mean/std\n",
    "\n",
    "        # Store the max SR per simulation\n",
    "        out_ = sr.max(axis=1).to_frame('max_SR')\n",
    "        out_['nTrials'] = nTrials_\n",
    "        out = pd.concat([out, out_], ignore_index=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    # Settings\n",
    "    nSims = 1_000\n",
    "    meanSR = 0\n",
    "    stdSR = 1\n",
    "    nTrials = list(set(np.logspace(1, 6, 1000).astype(int)))\n",
    "    nTrials.sort()\n",
    "\n",
    "    # Theoretical max Sharpe Ratios\n",
    "    sr_theoretical = pd.Series(\n",
    "        {i: getExpectedMaxSR(i, meanSR=meanSR, stdSR=stdSR) for i in tqdm(nTrials, desc=\"Computing theoretical SR\")}\n",
    "    )\n",
    "\n",
    "    # Empirical distribution of max SRs\n",
    "    sr_empirical = getDistMaxSR(nSims=nSims, nTrials=nTrials, meanSR=meanSR, stdSR=stdSR)\n",
    "\n",
    "    # Group empirical data to compute mean per nTrials\n",
    "    sr_empirical_mean = sr_empirical.groupby(\"nTrials\")[\"max_SR\"].mean()\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sr_theoretical.index, sr_theoretical.values, label=\"Theoretical max SR\", lw=2)\n",
    "    plt.plot(sr_empirical_mean.index, sr_empirical_mean.values, label=\"Empirical max SR (mean over sims)\", lw=2, linestyle='--')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"Number of Trials (log scale)\")\n",
    "    plt.ylabel(\"Max Sharpe Ratio\")\n",
    "    plt.title(\"False Strategy Theorem: Theoretical vs Empirical Max SR\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there is a raising hurdle that the researcher must beat as he conducts more backtests. We can compare these experimental results with the results predicted by the False Strategy theorem. The comparison of these two results(experiments and theoretical) seems to indicate that the False Strategy theorem accurately estimates the expected maximum SR for the range of trials studied.\n",
    "\n",
    "We can also test the precision of the theorem's approximation. From this experiment, we can deduce that the standard deviations are relatively small, below 0.5% of the values forecasted by the theorem, and they become smaller as the number of trials raises. \n",
    "\n",
    "The main conclusion from the False Strategy theorem is that unless $\\max_k\\{\\hat{SR}_k\\} >> E[\\max_k\\{\\hat{SR}_k\\}]$, the discovered strategy is likely to be a false positive.\n",
    "\n",
    "If we can compute $E[\\max_k\\{\\hat{SR}_k\\}]$, we can use that value to set the null hypothesis that must be rejected to conclude that the performance of the strategy is statistically significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.7 The Deflated Sharpe Ratio\n",
    "\n",
    "### Deflated Sharpe Ratio (DSR)\n",
    "\n",
    "According to Bailey and López de Prado (2014), the **Deflated Sharpe Ratio** accounts for multiple testing, non-normality, and limited sample size. It is defined as:\n",
    "\n",
    "$$\n",
    "\\widehat{\\text{DSR}} = Z \\left[ \\frac{ \\left( \\widehat{\\text{SR}} - \\mathbb{E} \\left[ \\max_k \\{ \\widehat{\\text{SR}}_k \\} \\right] \\right) \\sqrt{T - 1} }{ \\sqrt{1 - \\hat{\\gamma}_3 \\widehat{\\text{SR}} + \\frac{\\hat{\\gamma}_4 - 1}{4} \\widehat{\\text{SR}}^2} } \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Interpretation**:  \n",
    "The deflated Sharpe ratio $ \\widehat{\\text{DSR}} $ represents the probability of observing a Sharpe ratio greater than or equal to $ \\widehat{\\text{SR}} $, under the null hypothesis that the **true Sharpe ratio is zero**, while:\n",
    "\n",
    "- Adjusting for **skewness** $ \\hat{\\gamma}_3 $,\n",
    "- Adjusting for **kurtosis** $ \\hat{\\gamma}_4 $,\n",
    "- Accounting for **sample length** T,\n",
    "- And correcting for **multiple testing** through the expectation term $ \\mathbb{E}[\\max_k \\{\\widehat{\\text{SR}}_k\\}] $.\n",
    "\n",
    "This makes it a robust tool for evaluating whether an investment strategy shows genuine skill or if its performance can be explained by luck.\n",
    "\n",
    "However, the calculation requires the estimation of $ \\mathbb{E}[\\max_k \\{\\widehat{\\text{SR}}_k\\}]$, which in turn requires the estimation of K and $V[\\{\\widehat{SR}_k\\}]$, which is where we turn to ML.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.1 Effective Number of Trials\n",
    "\n",
    "The False Strategy Theorem requires knowing the number of **independent** trials within a testing family. However, in practice, researchers often run **multiple correlated trials** for each strategy rather than truly independent ones. \n",
    "\n",
    "This results in groups of highly correlated trials, and these relationships can be visualized using a **block-structured correlation matrix**.\n",
    "\n",
    "For example, López de Prado analyzes a correlation matrix of 6,385 backtested return series. After clustering, the ONC (Optimal Number of Clusters) algorithm identifies **four distinct strategy clusters**. This leads to the conservative estimate:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[K] = 4\n",
    "$$\n",
    "\n",
    "where K is the effective number of independent strategies. \n",
    "\n",
    "This adjustment is essential, as the **true number of independent trials** is likely **smaller** than the number of all tested strategies due to their correlation structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7.2 Variance Across Trials (Simplified Summary)\n",
    "\n",
    "After clustering $N$ strategies into $K$ correlated groups using the ONC algorithm, we can treat each cluster as a **composite strategy**. This allows us to reduce the number of strategies from $N$ to $K \\ll N$.\n",
    "\n",
    "To estimate the variance of Sharpe ratios across these clusters, we form **cluster-level return series** $S_{k,t}$ using **minimum variance weighting** to avoid overweighting volatile trials.\n",
    "\n",
    "Let:\n",
    "- $C_k$: set of trials in cluster $k$\n",
    "- $\\Sigma_k$: covariance matrix of returns within cluster $C_k$\n",
    "- $r_{i,t}$: return series for strategy $i \\in C_k$\n",
    "- $w_{k,i}$: weight of strategy $i$ in cluster $k$\n",
    "\n",
    "The weights for minimum-variance aggregation are:\n",
    "\n",
    "$$\n",
    "\\{w_{k,i}\\}_{i \\in C_k} = \\frac{\\Sigma_k^{-1} \\mathbf{1}_k}{\\mathbf{1}_k' \\Sigma_k^{-1} \\mathbf{1}_k}\n",
    "$$\n",
    "\n",
    "Then the cluster return series is:\n",
    "\n",
    "$$\n",
    "S_{k,t} = \\sum_{i \\in C_k} w_{k,i} \\cdot r_{i,t}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Annualizing the Sharpe Ratios\n",
    "\n",
    "Since clusters may differ in trading frequency, we **annualize** their Sharpe Ratios to make them comparable.\n",
    "\n",
    "Let:\n",
    "- $T_k$: length of $S_{k,t}$\n",
    "- $\\text{FirstDate}_k$, $\\text{LastDate}_k$: start and end dates of cluster $k$\n",
    "\n",
    "Compute:\n",
    "\n",
    "$$\n",
    "\\text{Years}_k = \\frac{\\text{LastDate}_k - \\text{FirstDate}_k}{365.25}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Frequency}_k = \\frac{T_k}{\\text{Years}_k}\n",
    "$$\n",
    "\n",
    "Then the **annualized Sharpe Ratio** is:\n",
    "\n",
    "$$\n",
    "\\widehat{\\text{aSR}}_k = \\frac{\\mathbb{E}[S_{k,t}] \\cdot \\text{Frequency}_k}{\\sqrt{\\mathbb{V}[S_{k,t}] \\cdot \\text{Frequency}_k}} = \\widehat{\\text{SR}}_k \\cdot \\sqrt{\\text{Frequency}_k}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Variance Across Clusters\n",
    "\n",
    "We can now estimate the variance of the **annualized** Sharpe ratios across clusters and adjust back to the original frequency of a selected cluster $k^*$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\mathbb{V}[\\widehat{\\text{SR}}_k]] = \\frac{\\mathbb{V}[\\widehat{\\text{aSR}}_k]}{\\text{Frequency}_{k^*}}\n",
    "$$\n",
    "\n",
    "This gives us a fair estimate of the variance of Sharpe ratios **across clusters**, expressed in terms of the selected cluster’s betting frequency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Strategies to Estimate the Effective Number of Trials\n",
    "\n",
    "In practice, researchers often test hundreds or thousands of strategy variations — but many of these are **not truly independent**. For example, changing a moving average from 20 to 21 days yields nearly identical results. This leads to an **overestimation of discovery significance** unless corrected.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Insight\n",
    "\n",
    "Although we may test $ N = 1000 $ strategies, they may reflect only $ K \\ll N $ **truly distinct ideas**. To adjust for this, we use **clustering** to estimate the number of **effective independent trials**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step-by-Step Process\n",
    "\n",
    "1. **Run N strategies** and collect their return time series.\n",
    "2. **Compute the correlation matrix** of these return series (shape: $N \\times N$).\n",
    "3. **Apply a clustering algorithm** (e.g. hierarchical clustering or ONC) to group similar strategies.\n",
    "4. The algorithm partitions strategies into K clusters, where:\n",
    "   - Within each cluster: high correlation (same idea).\n",
    "   - Across clusters: low correlation (different ideas).\n",
    "5. Treat each cluster as a **composite strategy** by aggregating the returns of its members (e.g. via minimum variance weighting).\n",
    "6. Compute the **Sharpe ratio for each cluster**.\n",
    "7. Use the resulting K Sharpe ratios to:\n",
    "   - Estimate the variance across clusters.\n",
    "   - Compute a more realistic expected maximum Sharpe ratio.\n",
    "   - Use K in the **Deflated Sharpe Ratio (DSR)** formula instead of N.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Matters\n",
    "\n",
    "This approach prevents overfitting and corrects for **multiple testing bias**. Instead of falsely concluding that a strategy is exceptional based on N trials, we correctly evaluate it based on the **true number of independent ideas**, K .\n",
    "\n",
    "For example:\n",
    "\n",
    "- \"What is the probability of getting a Sharpe ratio of 2.1 from 1,000 tests?\"  \n",
    "is replaced by:  \n",
    "- \"What is the probability of getting a Sharpe ratio of 2.1 from **4 truly independent** tests?\"\n",
    "\n",
    "This is the foundation of the **False Strategy Theorem** and helps avoid false discoveries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.9 Conclusions:\n",
    "\n",
    "The Sharpe ratio of an investment strategy under a single trial follows a Gaussian distribution, even if the strategy returns are non-Normal (still, returns must be stationary and ergodic).\n",
    "\n",
    "Researchers typically conduct a multiplicity of trials, and selecting out of them the best performing strategy increases the probability of selecting a false strategy. In this section, we have studied an alternative procedure to evaluate the extent to which testing set overfitting invalidates a discovered investment strategy.\n",
    "\n",
    "This approach relies on the False Strategy theorem. This theorem derives the expected value of the maximum Sharpe ratio as a function of the number of trials, K, and the variance of the Sharpe ratios across the trials. ML methods allow us to estimate these two variables. With this estimate of the expected value of max sharpe ratio, we can test whetehr the max SR is statistically significant using the deflated SR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 – False Strategy Theorem Demonstration\n",
    "\n",
    "This exercise investigates the dangers of data snooping and model overfitting by simulating a simple moving average strategy on S&P 500 data. The goal is to evaluate how the **maximum Sharpe ratio** observed from a large number of parameter combinations can lead to misleading performance claims.\n",
    "\n",
    "---\n",
    "\n",
    "#### a) Generate 1,000 strategy return series\n",
    "\n",
    "Simulate 1,000 variations of a moving average crossover strategy on S&P 500 price data by changing:\n",
    "\n",
    "- **i. Short lookback**: the number of days for the short-term moving average\n",
    "- **ii. Long lookback**: the number of days for the long-term moving average\n",
    "- **iii. Stop-loss threshold**: percent drop triggering exit\n",
    "- **iv. Profit-taking threshold**: percent rise triggering exit\n",
    "- **v. Maximum holding period**: max number of days to stay in a position\n",
    "\n",
    "Each variation corresponds to a different set of these 5 parameters.\n",
    "\n",
    "---\n",
    "\n",
    "#### b) Compute the maximum Sharpe ratio\n",
    "\n",
    "From the 1,000 strategies, compute the **Sharpe ratio** of each and report:\n",
    "\n",
    "$$\n",
    "\\max_k \\{ \\widehat{SR}_k \\}\n",
    "$$\n",
    "\n",
    "This simulates what a researcher might report if they cherry-pick the best-looking strategy from a large backtest.\n",
    "\n",
    "---\n",
    "\n",
    "#### c) Derive the expected maximum Sharpe ratio\n",
    "\n",
    "Compute the theoretical expected maximum Sharpe ratio under the **null hypothesis of no skill**, using:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[ \\max_k \\{ \\widehat{SR}_k \\} \\right] \\approx \\text{As described in Section 8.7}\n",
    "$$\n",
    "\n",
    "This represents the Sharpe ratio we would expect **just by chance** given 1,000 trials.\n",
    "\n",
    "---\n",
    "\n",
    "#### d) Compute the probability of observing the Sharpe ratio from (b)\n",
    "\n",
    "Calculate the probability that a Sharpe ratio at least as high as the one in (b) would be observed **by chance**, using the **Deflated Sharpe Ratio (DSR)** framework:\n",
    "\n",
    "$$\n",
    "\\widehat{\\text{DSR}} = Z\\left[ \\frac{ \\left( \\widehat{SR} - \\mathbb{E} \\left[ \\max_k \\{ \\widehat{SR}_k \\} \\right] \\right) \\sqrt{T - 1} }{ \\sqrt{1 - \\hat{\\gamma}_3 \\widehat{SR} + \\frac{\\hat{\\gamma}_4 - 1}{4} \\widehat{SR}^2} } \\right]\n",
    "$$\n",
    "\n",
    "This tells us whether the \"best\" strategy from part (b) is **statistically significant**, or just a **false positive** due to multiple testing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
