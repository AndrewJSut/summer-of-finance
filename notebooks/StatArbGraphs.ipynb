{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Arbitrage using Graph Theory\n",
    "\n",
    "Implmentation of \"Statistical arbitrage in multi-pair trading strategy based on graph clustering algorithms in US equities market\" by Adam Korniejczuka & Robert Slepaczukb.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methodology:\n",
    "\n",
    "1. Data Collection\n",
    "- Universe: S&P 500 constituents (updated historically as they change).\n",
    "- Timeframe: Daily data from 2000 to 2022.\n",
    "- Price Data: Adjusted close prices for all stocks.\n",
    "\n",
    "2. Graph Based clustering of the stocks\n",
    "- Calculate residual returns (on Fama-French 3 factor model)\n",
    "- Compute correlation matrix of residuals over a rolling window (30 days in paper)\n",
    "- Treat the correlation matrix as an adjacency matrix of a signed, weighted, undirected graph.\n",
    "- Use SPONGE-sym clustering algorithm to identify clusters of tightly connected stocks.\n",
    "\n",
    "3. Signal Generation\n",
    "- Every k days (e.g., every 10 days) for each cluster:\n",
    "- Compute cluster mean return over past 5 days.\n",
    "- Signal is then Stock’s 5-day return - Cluster’s 5-day mean return\n",
    "- If signal is significantly:\n",
    "    - Below cluster average then Long\n",
    "    - Above cluster average then Short\n",
    "\n",
    "4. Feature Engineering for Signal Quality\n",
    "- For each signal generated extract features like:\n",
    "- Graph-based:\n",
    "    - Local/global vertex degree\n",
    "    - Cluster size\n",
    "    - Graph density\n",
    "    - Cluster size\n",
    "- Price-based:\n",
    "    - Signal value\n",
    "    - Sign of deviation\n",
    "    - Stock & cluster average returns over past 10 days\n",
    "\n",
    "5. Label Signals\n",
    "- Label based on profitability (could try Triple Barrier Method as well), paper used \"If the stock’s return after the signal exceeds a threshold (e.g., 4%) or beats transaction cost.\"\n",
    "- This becomes a target for ML Classification\n",
    "\n",
    "6. Machine Learning Signal Classifier\n",
    "- Train multiple classifiers on the labeled dataset:\n",
    "    - Logistic Regression\n",
    "    - Gradient Boosted Trees\n",
    "    - Neural Net (MLP)\n",
    "    - SGD Classifier\n",
    "    - AdaBoost\n",
    "- Ensemble (soft voting) using classifier probabilities.\n",
    "\n",
    "7. Trade Filtering \n",
    "- Only trade signals where ensemble confidence > 0.6 to reduce bad trades and minimize transaction costs.\n",
    "\n",
    "8. Bet Sizing\n",
    "- Use Kelly Critereon\n",
    "\n",
    "9. Apply time-decaying take profit and stop loss:\n",
    "- Threshold shrinks each day since entry.\n",
    "- Threshold also weighted by signal confidence.\n",
    "\n",
    "10. Performance Evaluation\n",
    "- Backtest using realistic assumptions:\n",
    "    - Transaction cost = 0.05%\n",
    "    - Fractional shares allowed\n",
    "- Metrics:\n",
    "    - Annualized Return\n",
    "    - Sharpe & Sortino Ratio\n",
    "    - Max Drawdown\n",
    "    - Calmar Ratio\n",
    "    - Modified Information Ratios (IR*, IR**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  100 of 100 completed\n"
     ]
    }
   ],
   "source": [
    "# Smaller Dataset (to start)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "tickers = [\n",
    "    \"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"NVDA\", \"TSLA\", \"BRK-B\", \"JPM\", \"JNJ\",\n",
    "    \"V\", \"PG\", \"MA\", \"UNH\", \"HD\", \"XOM\", \"LLY\", \"ABBV\", \"AVGO\", \"KO\",\n",
    "    \"PEP\", \"CVX\", \"MRK\", \"COST\", \"WMT\", \"MCD\", \"BAC\", \"ADBE\", \"TMO\", \"ABT\",\n",
    "    \"CSCO\", \"ORCL\", \"ACN\", \"INTC\", \"CMCSA\", \"CRM\", \"NKE\", \"TXN\", \"DHR\", \"AMD\",\n",
    "    \"LIN\", \"PM\", \"NEE\", \"UPS\", \"BMY\", \"MS\", \"UNP\", \"LOW\", \"AMGN\", \"RTX\",\n",
    "    \"CAT\", \"HON\", \"GS\", \"SCHW\", \"QCOM\", \"AMAT\", \"BLK\", \"CVS\", \"MDT\", \"INTU\",\n",
    "    \"DE\", \"ISRG\", \"GE\", \"LMT\", \"BA\", \"ADI\", \"TGT\", \"SBUX\", \"ZTS\", \"GILD\",\n",
    "    \"PLD\", \"SPGI\", \"MO\", \"CI\", \"SO\", \"ADP\", \"NOW\", \"VRTX\", \"MMC\", \"CB\",\n",
    "    \"C\", \"REGN\", \"PNC\", \"CL\", \"PGR\", \"SYK\", \"USB\", \"TFC\", \"BDX\", \"BKNG\",\n",
    "    \"ETN\", \"ICE\", \"EQIX\", \"EL\", \"AON\", \"FIS\", \"HUM\", \"FDX\", \"GM\", \"APD\"\n",
    "]\n",
    "\n",
    "\n",
    "end_date = datetime.today()\n",
    "start_date = end_date - timedelta(days=365 * 2)\n",
    "\n",
    "# Download adjusted close prices\n",
    "df_prices = yf.download(tickers, start=start_date, end=end_date)[\"Close\"]\n",
    "\n",
    "# Handle missing data\n",
    "df_prices = df_prices.dropna(axis=1, thresh=len(df_prices) * 0.9)\n",
    "df_prices = df_prices.ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price to Returns\n",
    "def compute_log_returns(price_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute returns from a price DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure index is datetime\n",
    "    price_df.index = pd.to_datetime(price_df.index)\n",
    "    \n",
    "    # Calculate log returns\n",
    "    returns_df = price_df / price_df.shift(1)\n",
    "    \n",
    "    # Drop rows with NaNs introduced by shift\n",
    "    returns_df = np.log(price_df / price_df.shift(1)).dropna()\n",
    "    \n",
    "    return returns_df\n",
    "\n",
    "# Compute returns\n",
    "\n",
    "df_returns = compute_log_returns(df_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fama-French daily 3-factor data\n",
    "ff_factors = pd.read_csv(\"F-F_Research_Data_Factors_daily.CSV\", skiprows=3, index_col=0)\n",
    "\n",
    "# Use first row as header\n",
    "ff_factors = ff_factors.drop(ff_factors.index[0])\n",
    "\n",
    "ff_factors.dropna(inplace=True)\n",
    "ff_factors.columns = ['MKT', 'SMB', 'HML', 'RF']\n",
    "# Ensure all factor columns are float\n",
    "ff_factors[[\"MKT\", \"SMB\", \"HML\", \"RF\"]] = ff_factors[[\"MKT\", \"SMB\", \"HML\", \"RF\"]].astype(float)\n",
    "ff_factors.index = pd.to_datetime(ff_factors.index, format=\"%Y%m%d\")\n",
    "\n",
    "# Align returns and factors\n",
    "df_returns_ff = df_returns.join(ff_factors[[\"MKT\", \"SMB\", \"HML\", \"RF\"]], how=\"inner\")\n",
    "\n",
    "# Subtract RF from stock returns (excess returns only on stock columns)\n",
    "stock_cols = df_returns.columns\n",
    "excess_returns = df_returns_ff[stock_cols].sub(df_returns_ff[\"RF\"], axis=0)\n",
    "factors = df_returns_ff[[\"MKT\", \"SMB\", \"HML\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def compute_residuals(excess_returns, factors):\n",
    "    residuals = pd.DataFrame(index=excess_returns.index, columns=excess_returns.columns)\n",
    "    model = LinearRegression()\n",
    "\n",
    "    for ticker in excess_returns.columns:\n",
    "        y = excess_returns[ticker].dropna()\n",
    "        X = factors.loc[y.index]\n",
    "        model.fit(X, y)\n",
    "        if len(y) < 30:\n",
    "            continue  # skip short series\n",
    "        y_pred = model.predict(X)\n",
    "        residuals.loc[y.index, ticker] = y - y_pred\n",
    "\n",
    "    return residuals.astype(float)\n",
    "\n",
    "residuals = compute_residuals(excess_returns, factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Graph Based Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a dictionary, but could later convert to tensor if needed\n",
    "def rolling_signed_corr(residuals: pd.DataFrame, window: int = 30) -> dict:\n",
    "    \"\"\"\n",
    "    Compute rolling signed correlation matrices from residual returns.\n",
    "\n",
    "    Returns: \n",
    "    corr_matrices : dict\n",
    "        Dictionary mapping end-of-window date to signed correlation matrix.\n",
    "    \"\"\"\n",
    "    corr_matrices = {}\n",
    "    for end in range(window, len(residuals)):\n",
    "        window_data = residuals.iloc[end - window:end]\n",
    "        date_key = residuals.index[end]\n",
    "        corr = window_data.corr()\n",
    "        corr_matrices[date_key] = corr\n",
    "    return corr_matrices\n",
    "\n",
    "# Compute rolling signed correlation matrices\n",
    "rolling_corrs = rolling_signed_corr(residuals, window=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now cluster using SPONGEsym Algorithm\n",
    "\n",
    "# %pip install git+https://github.com/alan-turing-institute/SigNet.git\n",
    "from signet.cluster import Cluster\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "def run_sponge_sym(corr_matrix: pd.DataFrame, k: int = 6) -> dict:\n",
    "    \"\"\"\n",
    "    Applies SPONGE-sym clustering to a signed correlation matrix using SigNet.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    corr_matrix : pd.DataFrame\n",
    "        Signed correlation matrix (symmetric with values in [-1, 1]).\n",
    "    k : int\n",
    "        Number of clusters to generate.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    cluster_map : dict\n",
    "        Dictionary mapping tickers to cluster labels.\n",
    "    \"\"\"\n",
    "    W = corr_matrix.fillna(0).values\n",
    "    Ap = np.where(W > 0, W, 0)\n",
    "    An = np.where(W < 0, -W, 0)\n",
    "\n",
    "    Ap_sparse = csc_matrix(Ap)\n",
    "    An_sparse = csc_matrix(An)\n",
    "\n",
    "    c = Cluster((Ap_sparse, An_sparse))\n",
    "    labels = c.SPONGE_sym(k=k, tau_p=1.0, tau_n=1.0)  # ← correct function name\n",
    "\n",
    "    return dict(zip(corr_matrix.index, labels))\n",
    "\n",
    "sponge_cluster_results = {}\n",
    "\n",
    "for date, corr_matrix in rolling_corrs.items():\n",
    "    try:\n",
    "        clusters = run_sponge_sym(corr_matrix, k=6)\n",
    "        sponge_cluster_results[date] = clusters\n",
    "    except Exception as e:\n",
    "        print(f\"SPONGE clustering failed on {date}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried out other clustering methods, but SPONGE-sym is the most robust for signed graphs\n",
    "\n",
    "# Build the signed Laplacian Matrices\n",
    "def construct_signed_laplacian(corr_matrix: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construct the signed Laplacian matrix from a signed correlation matrix.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    corr_matrix : pd.DataFrame\n",
    "        Correlation matrix with values in [-1, 1].\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    laplacian : pd.DataFrame\n",
    "        Signed Laplacian matrix.\n",
    "    \"\"\"\n",
    "    W = corr_matrix.fillna(0).copy()\n",
    "    np.fill_diagonal(W.values, 0)  # zero diagonal\n",
    "    D = np.diag(np.sum(np.abs(W.values), axis=1))\n",
    "    L = D - W.values\n",
    "    return pd.DataFrame(L, index=W.index, columns=W.columns)\n",
    "\n",
    "laplacians = {\n",
    "    date: construct_signed_laplacian(corr)\n",
    "    for date, corr in rolling_corrs.items()\n",
    "}\n",
    "\n",
    "# Cluster the signed Laplacian matrices using KMeans variant found in \"ML for AM\" by MLdP\n",
    "\n",
    "from scipy.linalg import eigh\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def cluster_signed_graph_autoK(\n",
    "    laplacian: pd.DataFrame, \n",
    "    maxNumClusters: int = 10,\n",
    "    n_init: int = 10\n",
    ") -> tuple[dict, pd.Series, int]:\n",
    "    \"\"\"\n",
    "    Auto-selects optimal K and clusters stocks using spectral clustering on signed Laplacian.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    laplacian : pd.DataFrame\n",
    "        Signed Laplacian matrix.\n",
    "    maxNumClusters : int\n",
    "        Max number of clusters to try (searches from 2 to maxNumClusters).\n",
    "    n_init : int\n",
    "        KMeans initializations for robustness.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    clusters : dict\n",
    "        Mapping: ticker -> cluster label\n",
    "    silh_series : pd.Series\n",
    "        Silhouette scores for each stock under optimal K\n",
    "    best_k : int\n",
    "        Optimal number of clusters selected\n",
    "    \"\"\"\n",
    "    x = laplacian.values\n",
    "    tickers = laplacian.index\n",
    "\n",
    "    # Eigendecomposition once\n",
    "    eigvals, eigvecs = eigh(x)\n",
    "    scores, best_kmeans, best_k, best_silh = -np.inf, None, None, None\n",
    "\n",
    "    for k in range(2, maxNumClusters + 1):\n",
    "        embedding = eigvecs[:, :k]\n",
    "\n",
    "        for _ in range(n_init):\n",
    "            kmeans = KMeans(n_clusters=k, n_init=1, random_state=42)\n",
    "            labels = kmeans.fit_predict(embedding)\n",
    "            silh = silhouette_samples(embedding, labels)\n",
    "            t_stat = np.mean(silh) / (np.std(silh) + 1e-6)\n",
    "\n",
    "            if np.isnan(t_stat):\n",
    "                continue\n",
    "            if t_stat > scores:\n",
    "                scores = t_stat\n",
    "                best_kmeans = kmeans\n",
    "                best_k = k\n",
    "                best_silh = silh\n",
    "\n",
    "    final_labels = best_kmeans.labels_\n",
    "    cluster_map = dict(zip(tickers, final_labels))\n",
    "    silh_series = pd.Series(best_silh, index=tickers)\n",
    "\n",
    "    return cluster_map, silh_series, best_k\n",
    "\n",
    "cluster_results = {}\n",
    "\n",
    "for date, laplacian in laplacians.items():\n",
    "    try:\n",
    "        clusters, silh_scores, best_k = cluster_signed_graph_autoK(laplacian)\n",
    "        cluster_results[date] = {\n",
    "            \"clusters\": clusters,\n",
    "            \"silhouette_scores\": silh_scores,\n",
    "            \"best_k\": best_k\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to cluster on {date}: {e}\")\n",
    "\n",
    "best_k_series = pd.Series({\n",
    "    date: result[\"best_k\"]\n",
    "    for date, result in cluster_results.items()\n",
    "})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_k_series.sort_index().plot(marker='o')\n",
    "plt.title(\"Optimal Number of Clusters (best_k) Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Best k\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Signal Generation\n",
    "\n",
    "Every k days (e.g., every 10 days) for each cluster:\n",
    "- Compute cluster mean return over past 5 days.\n",
    "- Signal is then Stock’s 5-day return - Cluster’s 5-day mean return\n",
    "- If signal is significantly:\n",
    "    - Below cluster average then Long\n",
    "    - Above cluster average then Short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "signal_spacing = 10\n",
    "lookback_days = 10\n",
    "\n",
    "# If a stock outperformed its cluster its signal is positive\n",
    "# If a stock underperformed its cluster its signal is negative\n",
    "\n",
    "# Generate signal every `signal_spacing` days\n",
    "signal_dates = sorted(sponge_cluster_results.keys())[::signal_spacing]\n",
    "signals = []\n",
    "\n",
    "for date in signal_dates:\n",
    "    if date not in df_returns.index:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        date_idx = df_returns.index.get_loc(date)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "    window_start = date_idx - lookback_days\n",
    "    if window_start < 0:\n",
    "        continue  # skip if not enough data\n",
    "\n",
    "    past_window = df_returns.iloc[window_start:date_idx]\n",
    "    returns_past = past_window.sum()\n",
    "\n",
    "    # Cluster assignments from SPONGE\n",
    "    clusters = sponge_cluster_results[date]\n",
    "    cluster_series = pd.Series(clusters)\n",
    "\n",
    "    for cluster_id in set(cluster_series.values):\n",
    "        members = cluster_series[cluster_series == cluster_id].index\n",
    "\n",
    "        if len(members) < 2:\n",
    "            continue\n",
    "\n",
    "        cluster_mean = returns_past[members].mean()\n",
    "\n",
    "        for stock in members:\n",
    "            signal_value = returns_past[stock] - cluster_mean\n",
    "\n",
    "            signals.append({\n",
    "                \"date\": date,\n",
    "                \"stock\": stock,\n",
    "                \"cluster\": cluster_id,\n",
    "                \"signal\": signal_value,\n",
    "                \"cluster_mean\": cluster_mean,\n",
    "                \"stock_return\": returns_past[stock]\n",
    "            })\n",
    "\n",
    "# Create signal DataFrame\n",
    "signals_df = pd.DataFrame(signals)\n",
    "signals_df.sort_values([\"date\", \"signal\"], ascending=[True, False], inplace=True)\n",
    "signals_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Quantile-based signal thresholds\n",
    "lower, upper = signals_df[\"signal\"].quantile([0.10, 0.90])\n",
    "\n",
    "# Assign positions\n",
    "signals_df[\"position\"] = 0\n",
    "signals_df.loc[signals_df[\"signal\"] <= lower, \"position\"] = 1    # Long\n",
    "signals_df.loc[signals_df[\"signal\"] >= upper, \"position\"] = -1   # Short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "      <th>cluster</th>\n",
       "      <th>signal</th>\n",
       "      <th>cluster_mean</th>\n",
       "      <th>stock_return</th>\n",
       "      <th>position</th>\n",
       "      <th>check_signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>LLY</td>\n",
       "      <td>0</td>\n",
       "      <td>0.196621</td>\n",
       "      <td>-0.010575</td>\n",
       "      <td>0.186046</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.196621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>AMGN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.154840</td>\n",
       "      <td>-0.014895</td>\n",
       "      <td>0.139945</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.154840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>BKNG</td>\n",
       "      <td>2</td>\n",
       "      <td>0.117094</td>\n",
       "      <td>-0.014895</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.117094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>REGN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.099428</td>\n",
       "      <td>-0.014091</td>\n",
       "      <td>0.085337</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.099428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>PGR</td>\n",
       "      <td>1</td>\n",
       "      <td>0.096585</td>\n",
       "      <td>-0.014091</td>\n",
       "      <td>0.082494</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.096585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.083149</td>\n",
       "      <td>-0.031025</td>\n",
       "      <td>0.052124</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.083149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>CMCSA</td>\n",
       "      <td>4</td>\n",
       "      <td>0.056219</td>\n",
       "      <td>-0.028874</td>\n",
       "      <td>0.027345</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.056219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>GILD</td>\n",
       "      <td>2</td>\n",
       "      <td>0.045090</td>\n",
       "      <td>-0.014895</td>\n",
       "      <td>0.030195</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>GE</td>\n",
       "      <td>4</td>\n",
       "      <td>0.043063</td>\n",
       "      <td>-0.028874</td>\n",
       "      <td>0.014188</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>MRK</td>\n",
       "      <td>1</td>\n",
       "      <td>0.042354</td>\n",
       "      <td>-0.014091</td>\n",
       "      <td>0.028263</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>SYK</td>\n",
       "      <td>2</td>\n",
       "      <td>0.040212</td>\n",
       "      <td>-0.014895</td>\n",
       "      <td>0.025317</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>1</td>\n",
       "      <td>0.037282</td>\n",
       "      <td>-0.014091</td>\n",
       "      <td>0.023191</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>HUM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.036394</td>\n",
       "      <td>-0.014091</td>\n",
       "      <td>0.022302</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>CSCO</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036075</td>\n",
       "      <td>-0.028874</td>\n",
       "      <td>0.007201</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>XOM</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032178</td>\n",
       "      <td>-0.014091</td>\n",
       "      <td>0.018087</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>JNJ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.028582</td>\n",
       "      <td>-0.014091</td>\n",
       "      <td>0.014490</td>\n",
       "      <td>0</td>\n",
       "      <td>0.028582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>HD</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027787</td>\n",
       "      <td>-0.014091</td>\n",
       "      <td>0.013696</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>INTU</td>\n",
       "      <td>3</td>\n",
       "      <td>0.026195</td>\n",
       "      <td>-0.031025</td>\n",
       "      <td>-0.004830</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>COST</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026148</td>\n",
       "      <td>-0.014091</td>\n",
       "      <td>0.012057</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>ORCL</td>\n",
       "      <td>4</td>\n",
       "      <td>0.025672</td>\n",
       "      <td>-0.028874</td>\n",
       "      <td>-0.003203</td>\n",
       "      <td>0</td>\n",
       "      <td>0.025672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  stock  cluster    signal  cluster_mean  stock_return  position  \\\n",
       "0  2023-08-17    LLY        0  0.196621     -0.010575      0.186046        -1   \n",
       "1  2023-08-17   AMGN        2  0.154840     -0.014895      0.139945        -1   \n",
       "2  2023-08-17   BKNG        2  0.117094     -0.014895      0.102200        -1   \n",
       "3  2023-08-17   REGN        1  0.099428     -0.014091      0.085337        -1   \n",
       "4  2023-08-17    PGR        1  0.096585     -0.014091      0.082494        -1   \n",
       "5  2023-08-17   AMZN        3  0.083149     -0.031025      0.052124        -1   \n",
       "6  2023-08-17  CMCSA        4  0.056219     -0.028874      0.027345        -1   \n",
       "7  2023-08-17   GILD        2  0.045090     -0.014895      0.030195         0   \n",
       "8  2023-08-17     GE        4  0.043063     -0.028874      0.014188         0   \n",
       "9  2023-08-17    MRK        1  0.042354     -0.014091      0.028263         0   \n",
       "10 2023-08-17    SYK        2  0.040212     -0.014895      0.025317         0   \n",
       "11 2023-08-17    ZTS        1  0.037282     -0.014091      0.023191         0   \n",
       "12 2023-08-17    HUM        1  0.036394     -0.014091      0.022302         0   \n",
       "13 2023-08-17   CSCO        4  0.036075     -0.028874      0.007201         0   \n",
       "14 2023-08-17    XOM        1  0.032178     -0.014091      0.018087         0   \n",
       "15 2023-08-17    JNJ        1  0.028582     -0.014091      0.014490         0   \n",
       "16 2023-08-17     HD        1  0.027787     -0.014091      0.013696         0   \n",
       "17 2023-08-17   INTU        3  0.026195     -0.031025     -0.004830         0   \n",
       "18 2023-08-17   COST        1  0.026148     -0.014091      0.012057         0   \n",
       "19 2023-08-17   ORCL        4  0.025672     -0.028874     -0.003203         0   \n",
       "\n",
       "    check_signal  \n",
       "0       0.196621  \n",
       "1       0.154840  \n",
       "2       0.117094  \n",
       "3       0.099428  \n",
       "4       0.096585  \n",
       "5       0.083149  \n",
       "6       0.056219  \n",
       "7       0.045090  \n",
       "8       0.043063  \n",
       "9       0.042354  \n",
       "10      0.040212  \n",
       "11      0.037282  \n",
       "12      0.036394  \n",
       "13      0.036075  \n",
       "14      0.032178  \n",
       "15      0.028582  \n",
       "16      0.027787  \n",
       "17      0.026195  \n",
       "18      0.026148  \n",
       "19      0.025672  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signals_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering for Signal Quality\n",
    "- For each signal generated extract features:\n",
    "- Graph-based:\n",
    "    - Local/global vertex degree\n",
    "    - Cluster size\n",
    "    - Graph density\n",
    "    - Cluster size\n",
    "- Price-based:\n",
    "    - Signal value\n",
    "    - Sign of deviation\n",
    "    - Stock & cluster average returns over past 10 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding features to train ML on\n",
    "\n",
    "# Ensure returns are sorted by date\n",
    "df_returns = df_returns.sort_index()\n",
    "\n",
    "# Step 4: Enrich signals with features\n",
    "feature_rows = []\n",
    "\n",
    "for idx, row in signals_df.iterrows():\n",
    "    date = row['date']\n",
    "    stock = row['stock']\n",
    "    cluster_id = row['cluster']\n",
    "\n",
    "    # Retrieve cluster members\n",
    "    cluster_members = [s for s, c in sponge_cluster_results[date].items() if c == cluster_id]\n",
    "    if len(cluster_members) < 2 or stock not in cluster_members:\n",
    "        continue\n",
    "\n",
    "    # Retrieve correlation matrix for that date\n",
    "    corr_matrix = rolling_corrs.get(date)\n",
    "    if corr_matrix is None or stock not in corr_matrix.index:\n",
    "        continue\n",
    "\n",
    "    # 1. Local Degree: sum of abs correlations to others\n",
    "    local_degree = corr_matrix.loc[stock].drop(stock).abs().sum()\n",
    "\n",
    "    # 2. Cluster Density: average abs correlation between all cluster members\n",
    "    sub_corr = corr_matrix.loc[cluster_members, cluster_members]\n",
    "    tri_mask = np.triu(np.ones(sub_corr.shape), k=1).astype(bool)\n",
    "    flat_vals = sub_corr.values[tri_mask]\n",
    "    try:\n",
    "        density = np.nanmean(np.abs(flat_vals))  # alt: density = (2 * np.count_nonzero(flat_vals)) / (n * (n - 1))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # 3. Cluster Size\n",
    "    clust_size = len(cluster_members)\n",
    "\n",
    "    # 4. Price-Based Features: stock & cluster return over past 10 days\n",
    "    try:\n",
    "        date_idx = df_returns.index.get_loc(date)\n",
    "        window_start = date_idx - 10\n",
    "        if window_start < 0:\n",
    "            continue\n",
    "\n",
    "        past_window = df_returns.iloc[window_start:date_idx]\n",
    "        stock_10d_return = past_window[stock].sum()\n",
    "        cluster_10d_return = past_window[cluster_members].mean(axis=1).sum()\n",
    "    except Exception as e:\n",
    "        print(f\"Feature extraction failed for {date} - {stock}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Append enriched signal\n",
    "    feature_rows.append({\n",
    "        **row,\n",
    "        \"local_degree\": local_degree,\n",
    "        \"cluster_density\": density,\n",
    "        \"cluster_size\": clust_size,\n",
    "        \"signal_sign\": np.sign(row[\"signal\"]),\n",
    "        \"abs_signal\": np.abs(row[\"signal\"]),\n",
    "        \"stock_10d_return\": stock_10d_return,\n",
    "        \"cluster_10d_return\": cluster_10d_return\n",
    "    })\n",
    "\n",
    "# Build final DataFrame\n",
    "features_df = pd.DataFrame(feature_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "      <th>cluster</th>\n",
       "      <th>signal</th>\n",
       "      <th>cluster_mean</th>\n",
       "      <th>stock_return</th>\n",
       "      <th>position</th>\n",
       "      <th>local_degree</th>\n",
       "      <th>cluster_density</th>\n",
       "      <th>cluster_size</th>\n",
       "      <th>signal_sign</th>\n",
       "      <th>abs_signal</th>\n",
       "      <th>stock_10d_return</th>\n",
       "      <th>cluster_10d_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>LLY</td>\n",
       "      <td>0</td>\n",
       "      <td>0.196621</td>\n",
       "      <td>-0.010575</td>\n",
       "      <td>0.186046</td>\n",
       "      <td>-1</td>\n",
       "      <td>12.852767</td>\n",
       "      <td>0.207717</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.196621</td>\n",
       "      <td>0.186046</td>\n",
       "      <td>-0.010575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>AMGN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.154840</td>\n",
       "      <td>-0.014895</td>\n",
       "      <td>0.139945</td>\n",
       "      <td>-1</td>\n",
       "      <td>13.678793</td>\n",
       "      <td>0.286514</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.154840</td>\n",
       "      <td>0.139945</td>\n",
       "      <td>-0.014895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>BKNG</td>\n",
       "      <td>2</td>\n",
       "      <td>0.117094</td>\n",
       "      <td>-0.014895</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>-1</td>\n",
       "      <td>13.260046</td>\n",
       "      <td>0.286514</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.117094</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>-0.014895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>REGN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.099428</td>\n",
       "      <td>-0.014091</td>\n",
       "      <td>0.085337</td>\n",
       "      <td>-1</td>\n",
       "      <td>16.656741</td>\n",
       "      <td>0.252425</td>\n",
       "      <td>45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.099428</td>\n",
       "      <td>0.085337</td>\n",
       "      <td>-0.014091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>PGR</td>\n",
       "      <td>1</td>\n",
       "      <td>0.096585</td>\n",
       "      <td>-0.014091</td>\n",
       "      <td>0.082494</td>\n",
       "      <td>-1</td>\n",
       "      <td>16.386903</td>\n",
       "      <td>0.252425</td>\n",
       "      <td>45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.096585</td>\n",
       "      <td>0.082494</td>\n",
       "      <td>-0.014091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date stock  cluster    signal  cluster_mean  stock_return  position  \\\n",
       "0 2023-08-17   LLY        0  0.196621     -0.010575      0.186046        -1   \n",
       "1 2023-08-17  AMGN        2  0.154840     -0.014895      0.139945        -1   \n",
       "2 2023-08-17  BKNG        2  0.117094     -0.014895      0.102200        -1   \n",
       "3 2023-08-17  REGN        1  0.099428     -0.014091      0.085337        -1   \n",
       "4 2023-08-17   PGR        1  0.096585     -0.014091      0.082494        -1   \n",
       "\n",
       "   local_degree  cluster_density  cluster_size  signal_sign  abs_signal  \\\n",
       "0     12.852767         0.207717            14          1.0    0.196621   \n",
       "1     13.678793         0.286514            12          1.0    0.154840   \n",
       "2     13.260046         0.286514            12          1.0    0.117094   \n",
       "3     16.656741         0.252425            45          1.0    0.099428   \n",
       "4     16.386903         0.252425            45          1.0    0.096585   \n",
       "\n",
       "   stock_10d_return  cluster_10d_return  \n",
       "0          0.186046           -0.010575  \n",
       "1          0.139945           -0.014895  \n",
       "2          0.102200           -0.014895  \n",
       "3          0.085337           -0.014091  \n",
       "4          0.082494           -0.014091  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Label Signals\n",
    "- Label based on profitability \n",
    "- This becomes a target for ML Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idea 1 - Label the signals based on future return threshold\n",
    "\n",
    "labelled_rows = []\n",
    "\n",
    "profit_threshold = 0.03  # % threshold over 5-day holding\n",
    "holding_period = 10       # in days\n",
    "\n",
    "for _, row in features_df.iterrows():\n",
    "    date = row['date']\n",
    "    stock = row['stock']\n",
    "    position = row['position']  # -1 for short, 1 for long, 0 for neutral\n",
    "\n",
    "    try:\n",
    "        date_idx = df_returns.index.get_loc(date)\n",
    "        forward_window = df_returns.iloc[date_idx + 1: date_idx + 1 + holding_period]\n",
    "\n",
    "        # Cumulative log return → convert to standard return\n",
    "        log_ret_sum = forward_window[stock].sum()\n",
    "        future_return = np.exp(log_ret_sum) - 1\n",
    "\n",
    "        pnl = future_return * position\n",
    "        label = 1 if pnl > profit_threshold else 0\n",
    "\n",
    "        labelled_rows.append({\n",
    "            **row,\n",
    "            \"log_ret_sum\": log_ret_sum,\n",
    "            \"future_return\": future_return,\n",
    "            \"pnl\": pnl,\n",
    "            \"label\": label\n",
    "        })\n",
    "\n",
    "    except (KeyError, IndexError):\n",
    "        continue  # not enough forward data, skip\n",
    "\n",
    "labelled_df = pd.DataFrame(labelled_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Machine Learning Signal Classifier\n",
    "Train multiple classifiers on the labeled dataset:\n",
    "\n",
    "- Logistic Regression\n",
    "- Gradient Boosted Trees\n",
    "- Neural Net (MLP)\n",
    "- SGD Classifier\n",
    "- AdaBoost\n",
    "\n",
    "Ensemble (soft voting) using classifier probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Features to use\n",
    "feature_cols = [\n",
    "    'signal', 'abs_signal', 'signal_sign',\n",
    "    'local_degree', 'cluster_density', 'cluster_size',\n",
    "    'stock_10d_return', 'cluster_10d_return'\n",
    "]\n",
    "\n",
    "X = labelled_df[feature_cols]\n",
    "y = labelled_df['label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogReg...\n",
      "Training GBM...\n",
      "Training MLP...\n",
      "Training SGD...\n",
      "Training AdaBoost...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define base models\n",
    "models = {\n",
    "    \"LogReg\": make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000)),\n",
    "    \"GBM\": GradientBoostingClassifier(),\n",
    "    \"MLP\": make_pipeline(StandardScaler(), MLPClassifier(hidden_layer_sizes=(50, 20), max_iter=500)),\n",
    "    \"SGD\": make_pipeline(StandardScaler(), SGDClassifier(loss='log_loss', max_iter=1000)),\n",
    "    \"AdaBoost\": AdaBoostClassifier()\n",
    "}\n",
    "\n",
    "# Fit all base models\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating LogReg...\n",
      "AUC error for LogReg: y should be a 1d array, got an array of shape (1290, 2) instead.\n",
      "\n",
      "Evaluating GBM...\n",
      "AUC error for GBM: y should be a 1d array, got an array of shape (1290, 2) instead.\n",
      "\n",
      "Evaluating MLP...\n",
      "AUC error for MLP: y should be a 1d array, got an array of shape (1290, 2) instead.\n",
      "\n",
      "Evaluating SGD...\n",
      "AUC error for SGD: y should be a 1d array, got an array of shape (1290, 2) instead.\n",
      "\n",
      "Evaluating AdaBoost...\n",
      "AUC error for AdaBoost: y should be a 1d array, got an array of shape (1290, 2) instead.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import pandas as pd\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "        auc = roc_auc_score(y_test, y_proba, multi_class='ovr')\n",
    "    except Exception as e:\n",
    "        print(f\"AUC error for {name}: {e}\")\n",
    "        auc = None\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    results[name] = {\n",
    "        \"AUC\": auc,\n",
    "        \"Report\": report\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: LogReg\n",
      "AUC: Not available\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      1218\n",
      "           1       0.28      0.07      0.11        72\n",
      "\n",
      "    accuracy                           0.94      1290\n",
      "   macro avg       0.61      0.53      0.54      1290\n",
      "weighted avg       0.91      0.94      0.92      1290\n",
      "\n",
      "\n",
      "Model: GBM\n",
      "AUC: Not available\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      1218\n",
      "           1       0.48      0.14      0.22        72\n",
      "\n",
      "    accuracy                           0.94      1290\n",
      "   macro avg       0.71      0.56      0.59      1290\n",
      "weighted avg       0.92      0.94      0.93      1290\n",
      "\n",
      "\n",
      "Model: MLP\n",
      "AUC: Not available\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97      1218\n",
      "           1       0.37      0.18      0.24        72\n",
      "\n",
      "    accuracy                           0.94      1290\n",
      "   macro avg       0.66      0.58      0.61      1290\n",
      "weighted avg       0.92      0.94      0.93      1290\n",
      "\n",
      "\n",
      "Model: SGD\n",
      "AUC: Not available\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      1218\n",
      "           1       0.40      0.08      0.14        72\n",
      "\n",
      "    accuracy                           0.94      1290\n",
      "   macro avg       0.67      0.54      0.55      1290\n",
      "weighted avg       0.92      0.94      0.92      1290\n",
      "\n",
      "\n",
      "Model: AdaBoost\n",
      "AUC: Not available\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      1218\n",
      "           1       0.18      0.03      0.05        72\n",
      "\n",
      "    accuracy                           0.94      1290\n",
      "   macro avg       0.56      0.51      0.51      1290\n",
      "weighted avg       0.90      0.94      0.92      1290\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Display summaries\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    print(f\"AUC: {metrics['AUC']:.4f}\" if metrics['AUC'] is not None else \"AUC: Not available\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, models[name].predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Soft Voting Ensemble):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      1218\n",
      "           1       0.50      0.07      0.12        72\n",
      "\n",
      "    accuracy                           0.94      1290\n",
      "   macro avg       0.72      0.53      0.55      1290\n",
      "weighted avg       0.92      0.94      0.92      1290\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensemble\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(name, model) for name, model in models.items()],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(\"Classification Report (Soft Voting Ensemble):\\n\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced Methodology using Advancements in Financial Machine Learning by MLdP\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Data Collection (AFML Chapter 2)\n",
    "- **Universe**: Historical constituents of the S&P 500.\n",
    "- **Timeframe**: 2000–2022.\n",
    "- **Data Sampling**: Replace fixed daily bars with **Dollar Imbalance Bars** to reflect information flow more accurately and reduce sampling bias.  \n",
    "  *Added from AFML Chapter 2: Sampling Bars.*\n",
    "\n",
    "- **Returns**: Calculate **fractionally differenced returns** to achieve stationarity while preserving memory.  \n",
    "  *Added from AFML Chapter 5: Fractional Differencing.*\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Graph-Based Clustering (AFML Appendix A)\n",
    "- **Residual Estimation**: Run Fama-French 3-Factor model to extract residual returns.\n",
    "- **Dependency Estimation**: Replace Pearson correlation with **information-adjusted or shrinkage estimators** (e.g., Ledoit-Wolf).  \n",
    "  *AFML Appendix A: More robust covariance estimation.*\n",
    "\n",
    "- **Graph Construction**: Treat the (residual) correlation matrix as an adjacency matrix of a signed, weighted graph.\n",
    "- **Clustering**: Use **SPONGE-sym** or a robust spectral clustering method to identify clusters of tightly connected stocks.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Signal Generation\n",
    "- **Rebalance Frequency**: Every *k* days (e.g., every 10 days).\n",
    "- **Cluster Signal**: For each stock in a cluster:\n",
    "  - Compute 5-day return deviation from the cluster’s 5-day mean.\n",
    "  - **Long** if significantly below mean; **Short** if significantly above.\n",
    "- **Preprocessing**: Apply **fractional differencing** to returns before signal construction for improved stationarity.  \n",
    "  *AFML Chapter 5: Retaining memory while avoiding spurious relationships.*\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Feature Engineering for Signal Quality (AFML Chapters 5 & 9)\n",
    "- **Graph-based Features**:\n",
    "  - Local/global vertex degree\n",
    "  - Cluster size\n",
    "  - Graph density\n",
    "  - Cluster count relative to total nodes\n",
    "\n",
    "- **Price-based Features**:\n",
    "  - Deviation from cluster mean\n",
    "  - Sign of deviation\n",
    "  - Stock & cluster momentum (last 10 days)\n",
    "  - **Entropy of cluster returns** and **volatility of degree distribution**  \n",
    "    *Inspired by AFML’s emphasis on feature richness and signal structure (Ch. 9).*\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Labeling Signals (AFML Chapter 3)\n",
    "- Replace fixed return threshold with the **Triple Barrier Method**:\n",
    "  - **Upper barrier**: Profit target\n",
    "  - **Lower barrier**: Stop-loss\n",
    "  - **Vertical barrier**: Time-decay limit\n",
    "- Each label reflects whether the upper, lower, or time barrier was breached first.\n",
    "  *AFML Chapter 3: More objective, robust label generation.*\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Machine Learning Classifier (AFML Chapters 7–9)\n",
    "- **Models**: Train multiple classifiers including:\n",
    "  - Logistic Regression\n",
    "  - Gradient Boosted Trees\n",
    "  - MLP\n",
    "  - AdaBoost\n",
    "  - SGD Classifier\n",
    "\n",
    "- **Meta-Labeling**: Use the base signal as a feature and train classifiers to predict *whether to act on it*.  \n",
    "  *AFML Chapter 7: Meta-Labeling.*\n",
    "\n",
    "- **Model Selection**: Use **Nested Cross-Validation** to tune hyperparameters while preventing leakage.  \n",
    "  *AFML Chapter 8.*\n",
    "\n",
    "- **Feature Importance**: Analyze with both **Mean Decrease Accuracy (MDA)** and **Mean Decrease Impurity (MDI)** to ensure feature stability.  \n",
    "  *AFML Chapter 9.*\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Trade Filtering (AFML Chapter 7)\n",
    "- Use ensemble confidence from the meta-labeler.\n",
    "- Only act on signals with **predicted probability above the 90th percentile**, based on out-of-sample distribution.\n",
    "- **Probabilistic filtering** replaces static 0.6 threshold.  \n",
    "  *AFML Chapter 7.*\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Bet Sizing (AFML Chapter 10)\n",
    "- Replace classic Kelly with **bet sizing based on predicted probability**:\n",
    "  - `size = 2p - 1` where `p` is model confidence\n",
    "- Optionally implement **Dynamic Programming-based Bet Sizing** for optimal growth with risk constraints.  \n",
    "  *AFML Chapter 10.*\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Risk Management (AFML Chapter 3)\n",
    "- Use **Triple Barrier parameters** (profit-taking, stop-loss, time constraint) for exit strategy.\n",
    "- Thresholds decay over time or adapt to **volatility regime**.\n",
    "- Optional: dynamically tune barriers using recent volatility or entropy.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Performance Evaluation (AFML Chapter 11)\n",
    "- Metrics:\n",
    "  - Annualized Return (CAGR)\n",
    "  - Sharpe Ratio\n",
    "  - Sortino Ratio\n",
    "  - Max Drawdown\n",
    "  - Calmar Ratio\n",
    "  - Modified Information Ratios (IR*, IR**)\n",
    "\n",
    "- **Robustness Checks**:\n",
    "  - Use **Combinatorially Symmetric Cross-Validation (CSCV)** to assess overfitting and compute **Probability of Backtest Overfitting (PBO)**.  \n",
    "    *AFML Chapter 11.*\n",
    "\n",
    "- **Hypothesis Testing**:\n",
    "  - Apply **White’s Reality Check** or **bootstrapped performance testing** for statistical validity.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
