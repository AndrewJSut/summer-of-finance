{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Arbitrage using Graph Theory\n",
    "\n",
    "Implmentation of \"Statistical arbitrage in multi-pair trading strategy based on graph clustering algorithms in US equities market\" by Adam Korniejczuka & Robert Slepaczukb.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methodology:\n",
    "\n",
    "1. Data Collection\n",
    "- Universe: S&P 500 constituents (updated historically as they change).\n",
    "- Timeframe: Daily data from 2000 to 2022.\n",
    "- Price Data: Adjusted close prices for all stocks.\n",
    "\n",
    "2. Graph Based clustering of the stocks\n",
    "- Calculate residual returns (on Fama-French 3 factor model)\n",
    "- Compute correlation matrix of residuals over a rolling window (30 days in paper)\n",
    "- Treat the correlation matrix as an adjacency matrix of a signed, weighted, undirected graph.\n",
    "- Use SPONGE-sym clustering algorithm to identify clusters of tightly connected stocks.\n",
    "\n",
    "3. Signal Generation\n",
    "- Every k days (e.g., every 10 days) for each cluster:\n",
    "- Compute cluster mean return over past 5 days.\n",
    "- Signal is then Stock’s 5-day return - Cluster’s 5-day mean return\n",
    "- If signal is significantly:\n",
    "    - Below cluster average then Long\n",
    "    - Above cluster average then Short\n",
    "\n",
    "4. Feature Engineering for Signal Quality\n",
    "- For each signal generated extract features like:\n",
    "- Graph-based:\n",
    "    - Local/global vertex degree\n",
    "    - Cluster size\n",
    "    - Graph density\n",
    "    - Cluster size\n",
    "- Price-based:\n",
    "    - Signal value\n",
    "    - Sign of deviation\n",
    "    - Stock & cluster average returns over past 10 days\n",
    "\n",
    "5. Label Signals\n",
    "- Label based on profitability (could try Triple Barrier Method as well), paper used \"If the stock’s return after the signal exceeds a threshold (e.g., 4%) or beats transaction cost.\"\n",
    "- This becomes a target for ML Classification\n",
    "\n",
    "6. Machine Learning Signal Classifier\n",
    "- Train multiple classifiers on the labeled dataset:\n",
    "    - Logistic Regression\n",
    "    - Gradient Boosted Trees\n",
    "    - Neural Net (MLP)\n",
    "    - SGD Classifier\n",
    "    - AdaBoost\n",
    "- Ensemble (soft voting) using classifier probabilities.\n",
    "\n",
    "7. Trade Filtering \n",
    "- Only trade signals where ensemble confidence > 0.6 to reduce bad trades and minimize transaction costs.\n",
    "\n",
    "8. Bet Sizing\n",
    "- Use Kelly Critereon\n",
    "\n",
    "9. Apply time-decaying take profit and stop loss:\n",
    "- Threshold shrinks each day since entry.\n",
    "- Threshold also weighted by signal confidence.\n",
    "\n",
    "10. Performance Evaluation\n",
    "- Backtest using realistic assumptions:\n",
    "    - Transaction cost = 0.05%\n",
    "    - Fractional shares allowed\n",
    "- Metrics:\n",
    "    - Annualized Return\n",
    "    - Sharpe & Sortino Ratio\n",
    "    - Max Drawdown\n",
    "    - Calmar Ratio\n",
    "    - Modified Information Ratios (IR*, IR**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  501 of 501 completed\n"
     ]
    }
   ],
   "source": [
    "# Smaller Dataset (to start)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sp500 = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\")[0]\n",
    "tickers = sp500['Symbol'].tolist()\n",
    "\n",
    "# Remove RF (confuses with risk free rate later)\n",
    "tickers = [ticker for ticker in tickers if ticker != 'RF']\n",
    "\n",
    "#Change any . to -\n",
    "tickers = [ticker.replace('.', '-') for ticker in tickers]\n",
    "\n",
    "start_date = \"2014-01-01\"\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "df_data = yf.download(\n",
    "    tickers,\n",
    "    start=start_date,\n",
    "    end=end_date,\n",
    "    progress=True,\n",
    "    group_by='ticker',\n",
    "    auto_adjust=True,\n",
    "    threads=True\n",
    ")\n",
    "\n",
    "# Extract just Close prices into a single DataFrame\n",
    "df_prices = pd.concat(\n",
    "    [df_data[ticker]['Close'].rename(ticker) for ticker in tickers if ticker in df_data],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "threshold = int(0.95 * len(df_prices))  # Allow up to 5% NaNs\n",
    "df_prices = df_prices.dropna(axis=1, thresh=threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price to Returns\n",
    "def compute_log_returns(price_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute returns from a price DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure index is datetime\n",
    "    price_df.index = pd.to_datetime(price_df.index)\n",
    "    \n",
    "    # Calculate log returns\n",
    "    returns_df = price_df / price_df.shift(1)\n",
    "    \n",
    "    # Drop rows with NaNs introduced by shift\n",
    "    returns_df = np.log(price_df / price_df.shift(1)).dropna()\n",
    "    \n",
    "    return returns_df\n",
    "\n",
    "# Compute returns\n",
    "\n",
    "df_returns = compute_log_returns(df_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fama-French daily 3-factor data\n",
    "ff_factors = pd.read_csv(\"F-F_Research_Data_Factors_daily.CSV\", skiprows=3, index_col=0)\n",
    "\n",
    "# Use first row as header\n",
    "ff_factors = ff_factors.drop(ff_factors.index[0])\n",
    "\n",
    "ff_factors.dropna(inplace=True)\n",
    "ff_factors.columns = ['MKT', 'SMB', 'HML', 'RF']\n",
    "# Ensure all factor columns are float\n",
    "ff_factors[[\"MKT\", \"SMB\", \"HML\", \"RF\"]] = ff_factors[[\"MKT\", \"SMB\", \"HML\", \"RF\"]].astype(float)\n",
    "ff_factors.index = pd.to_datetime(ff_factors.index, format=\"%Y%m%d\")\n",
    "\n",
    "# Align returns and factors\n",
    "df_returns_ff = df_returns.join(ff_factors[[\"MKT\", \"SMB\", \"HML\", \"RF\"]], how=\"inner\")\n",
    "\n",
    "# Subtract RF from stock returns (excess returns only on stock columns)\n",
    "stock_cols = df_returns.columns\n",
    "excess_returns = df_returns_ff[stock_cols].sub(df_returns_ff[\"RF\"], axis=0)\n",
    "factors = df_returns_ff[[\"MKT\", \"SMB\", \"HML\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def compute_residuals(excess_returns, factors):\n",
    "    residuals = pd.DataFrame(index=excess_returns.index, columns=excess_returns.columns)\n",
    "    model = LinearRegression()\n",
    "\n",
    "    for ticker in excess_returns.columns:\n",
    "        y = excess_returns[ticker].dropna()\n",
    "        X = factors.loc[y.index]\n",
    "        model.fit(X, y)\n",
    "        if len(y) < 30:\n",
    "            continue  # skip short series\n",
    "        y_pred = model.predict(X)\n",
    "        residuals.loc[y.index, ticker] = y - y_pred\n",
    "\n",
    "    return residuals.astype(float)\n",
    "\n",
    "residuals = compute_residuals(excess_returns, factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Graph Based Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a dictionary, but could later convert to tensor if needed\n",
    "def rolling_signed_corr(residuals: pd.DataFrame, window: int = 60) -> dict:\n",
    "    \"\"\"\n",
    "    Compute rolling signed correlation matrices from residual returns.\n",
    "\n",
    "    Returns: \n",
    "    corr_matrices : dict\n",
    "        Dictionary mapping end-of-window date to signed correlation matrix.\n",
    "    \"\"\"\n",
    "    corr_matrices = {}\n",
    "    for end in range(window, len(residuals)):\n",
    "        window_data = residuals.iloc[end - window:end]\n",
    "        date_key = residuals.index[end]\n",
    "        corr = window_data.corr()\n",
    "        corr_matrices[date_key] = corr\n",
    "    return corr_matrices\n",
    "\n",
    "# Compute rolling signed correlation matrices\n",
    "rolling_corrs = rolling_signed_corr(residuals, window=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now cluster using SPONGEsym Algorithm\n",
    "\n",
    "# %pip install git+https://github.com/alan-turing-institute/SigNet.git\n",
    "from signet.cluster import Cluster\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "def estimate_k_variance(corr_matrix, threshold=0.90):\n",
    "    eigenvalues = np.linalg.eigvalsh(corr_matrix.fillna(0).values)\n",
    "    eigenvalues = np.flip(np.sort(eigenvalues))  # descending order\n",
    "    cumulative_variance = np.cumsum(eigenvalues) / np.sum(eigenvalues)\n",
    "    k = np.searchsorted(cumulative_variance, threshold) + 1\n",
    "    return k\n",
    "\n",
    "def run_sponge_sym(corr_matrix: pd.DataFrame, k: int = 6) -> dict:\n",
    "    \"\"\"\n",
    "    Applies SPONGE-sym clustering to a signed correlation matrix using SigNet.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    corr_matrix : pd.DataFrame\n",
    "        Signed correlation matrix (symmetric with values in [-1, 1]).\n",
    "    k : int\n",
    "        Number of clusters to generate.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    cluster_map : dict\n",
    "        Dictionary mapping tickers to cluster labels.\n",
    "    \"\"\"\n",
    "    W = corr_matrix.fillna(0).values\n",
    "    Ap = np.where(W > 0, W, 0)\n",
    "    An = np.where(W < 0, -W, 0)\n",
    "\n",
    "    Ap_sparse = csc_matrix(Ap)\n",
    "    An_sparse = csc_matrix(An)\n",
    "\n",
    "    c = Cluster((Ap_sparse, An_sparse))\n",
    "    labels = c.SPONGE_sym(k=k, tau_p=1.0, tau_n=1.0)  # ← correct function name\n",
    "\n",
    "    return dict(zip(corr_matrix.index, labels))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"signet.cluster\")\n",
    "\n",
    "sponge_cluster_results = {}\n",
    "\n",
    "for date, corr_matrix in rolling_corrs.items():\n",
    "    try:\n",
    "        k_est = estimate_k_variance(corr_matrix, threshold=0.90)\n",
    "        clusters = run_sponge_sym(corr_matrix, k=k_est)\n",
    "        sponge_cluster_results[date] = clusters\n",
    "    except Exception as e:\n",
    "        print(f\"SPONGE clustering failed on {date}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Signal Generation\n",
    "\n",
    "Every k days (e.g., every 10 days) for each cluster:\n",
    "- Compute cluster mean return over past 5 days.\n",
    "- Signal is then Stock’s 5-day return - Cluster’s 5-day mean return\n",
    "- If signal is significantly:\n",
    "    - Below cluster average then Long\n",
    "    - Above cluster average then Short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "signal_spacing = 10\n",
    "lookback_days = 10\n",
    "\n",
    "# If a stock outperformed its cluster its signal is positive\n",
    "# If a stock underperformed its cluster its signal is negative\n",
    "\n",
    "# Generate signal every `signal_spacing` days\n",
    "signal_dates = sorted(sponge_cluster_results.keys())[::signal_spacing]\n",
    "signals = []\n",
    "\n",
    "for date in signal_dates:\n",
    "    if date not in df_returns.index:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        date_idx = df_returns.index.get_loc(date)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "    window_start = date_idx - lookback_days\n",
    "    if window_start < 0:\n",
    "        continue  # skip if not enough data\n",
    "\n",
    "    past_window = df_returns.iloc[window_start:date_idx]\n",
    "    returns_past = past_window.sum()\n",
    "\n",
    "    # Cluster assignments from SPONGE\n",
    "    clusters = sponge_cluster_results[date]\n",
    "    cluster_series = pd.Series(clusters)\n",
    "\n",
    "    for cluster_id in set(cluster_series.values):\n",
    "        members = cluster_series[cluster_series == cluster_id].index\n",
    "\n",
    "        if len(members) < 2:\n",
    "            continue\n",
    "\n",
    "        cluster_mean = returns_past[members].mean()\n",
    "\n",
    "        for stock in members:\n",
    "            signal_value = returns_past[stock] - cluster_mean\n",
    "\n",
    "            signals.append({\n",
    "                \"date\": date,\n",
    "                \"stock\": stock,\n",
    "                \"cluster\": cluster_id,\n",
    "                \"signal\": signal_value,\n",
    "                \"cluster_mean\": cluster_mean,\n",
    "                \"stock_return\": returns_past[stock]\n",
    "            })\n",
    "\n",
    "# Create signal DataFrame\n",
    "signals_df = pd.DataFrame(signals)\n",
    "signals_df.sort_values([\"date\", \"signal\"], ascending=[True, False], inplace=True)\n",
    "signals_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Quantile-based signal thresholds\n",
    "lower, upper = signals_df[\"signal\"].quantile([0.10, 0.90])\n",
    "\n",
    "# Assign positions\n",
    "signals_df[\"position\"] = 0\n",
    "signals_df.loc[signals_df[\"signal\"] <= lower, \"position\"] = 1    # Long\n",
    "signals_df.loc[signals_df[\"signal\"] >= upper, \"position\"] = -1   # Short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "      <th>cluster</th>\n",
       "      <th>signal</th>\n",
       "      <th>cluster_mean</th>\n",
       "      <th>stock_return</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-09-15</td>\n",
       "      <td>PANW</td>\n",
       "      <td>3</td>\n",
       "      <td>0.169870</td>\n",
       "      <td>0.025216</td>\n",
       "      <td>0.195085</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-09-15</td>\n",
       "      <td>ENPH</td>\n",
       "      <td>12</td>\n",
       "      <td>0.157938</td>\n",
       "      <td>0.025218</td>\n",
       "      <td>0.183156</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-09-15</td>\n",
       "      <td>AXON</td>\n",
       "      <td>7</td>\n",
       "      <td>0.137672</td>\n",
       "      <td>0.024560</td>\n",
       "      <td>0.162232</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-09-15</td>\n",
       "      <td>ULTA</td>\n",
       "      <td>12</td>\n",
       "      <td>0.134709</td>\n",
       "      <td>0.025218</td>\n",
       "      <td>0.159926</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-09-15</td>\n",
       "      <td>AVGO</td>\n",
       "      <td>5</td>\n",
       "      <td>0.117919</td>\n",
       "      <td>0.022253</td>\n",
       "      <td>0.140172</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date stock  cluster    signal  cluster_mean  stock_return  position\n",
       "0 2014-09-15  PANW        3  0.169870      0.025216      0.195085        -1\n",
       "1 2014-09-15  ENPH       12  0.157938      0.025218      0.183156        -1\n",
       "2 2014-09-15  AXON        7  0.137672      0.024560      0.162232        -1\n",
       "3 2014-09-15  ULTA       12  0.134709      0.025218      0.159926        -1\n",
       "4 2014-09-15  AVGO        5  0.117919      0.022253      0.140172        -1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signals_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering for Signal Quality\n",
    "- For each signal generated extract features:\n",
    "- Graph-based:\n",
    "    - Local/global vertex degree\n",
    "    - Cluster size\n",
    "    - Graph density\n",
    "    - Cluster size\n",
    "- Price-based:\n",
    "    - Signal value\n",
    "    - Sign of deviation\n",
    "    - Stock & cluster average returns over past 10 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding features to train ML on\n",
    "\n",
    "# Ensure returns are sorted by date\n",
    "df_returns = df_returns.sort_index()\n",
    "\n",
    "# Enrich signals with features\n",
    "feature_rows = []\n",
    "\n",
    "for idx, row in signals_df.iterrows():\n",
    "    date = row['date']\n",
    "    stock = row['stock']\n",
    "    cluster_id = row['cluster']\n",
    "\n",
    "    # Retrieve cluster members\n",
    "    cluster_members = [s for s, c in sponge_cluster_results[date].items() if c == cluster_id]\n",
    "    if len(cluster_members) < 2 or stock not in cluster_members:\n",
    "        continue\n",
    "\n",
    "    # Retrieve correlation matrix for that date\n",
    "    corr_matrix = rolling_corrs.get(date)\n",
    "    if corr_matrix is None or stock not in corr_matrix.index:\n",
    "        continue\n",
    "\n",
    "    # 1. Local Degree: sum of abs correlations to others\n",
    "    local_degree = corr_matrix.loc[stock].drop(stock).abs().sum()\n",
    "\n",
    "    # 2. Cluster Density: average abs correlation between all cluster members\n",
    "    sub_corr = corr_matrix.loc[cluster_members, cluster_members]\n",
    "    tri_mask = np.triu(np.ones(sub_corr.shape), k=1).astype(bool)\n",
    "    flat_vals = sub_corr.values[tri_mask]\n",
    "    try:\n",
    "        density = np.nanmean(np.abs(flat_vals))  # alt: density = (2 * np.count_nonzero(flat_vals)) / (n * (n - 1))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # 3. Cluster Size\n",
    "    clust_size = len(cluster_members)\n",
    "\n",
    "   # 4. Global (normalised) vertex degree: mean |ρ| of the stock to every other stock in the graph\n",
    "    global_degree = corr_matrix.loc[stock].drop(stock).abs().mean()\n",
    "\n",
    "    # 5. “Number-of-clusters ÷ graph size” – graph-level normalised feature\n",
    "    clusters_per_graph = len(set(sponge_cluster_results[date].values())) / corr_matrix.shape[0]\n",
    "\n",
    "    # 6. Price-Based Features: stock & cluster return over past 10 days\n",
    "    try:\n",
    "        date_idx = df_returns.index.get_loc(date)\n",
    "        window_start = date_idx - 10\n",
    "        if window_start < 0:\n",
    "            continue\n",
    "\n",
    "        past_window = df_returns.iloc[window_start:date_idx]\n",
    "        stock_10d_return = past_window[stock].sum()\n",
    "        cluster_10d_return = past_window[cluster_members].mean(axis=1).sum()\n",
    "    except Exception as e:\n",
    "        print(f\"Feature extraction failed for {date} - {stock}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Append enriched signal\n",
    "    feature_rows.append({\n",
    "        **row,\n",
    "        \"local_degree\": local_degree,\n",
    "        \"cluster_density\": density,\n",
    "        \"cluster_size\": clust_size,\n",
    "        \"global_degree\": global_degree,\n",
    "        \"clusters_per_graph\": clusters_per_graph,\n",
    "        \"signal_sign\": np.sign(row[\"signal\"]),\n",
    "        \"abs_signal\": np.abs(row[\"signal\"]),\n",
    "        \"stock_10d_return\": stock_10d_return,\n",
    "        \"cluster_10d_return\": cluster_10d_return\n",
    "    })\n",
    "\n",
    "\n",
    "# Build final DataFrame\n",
    "features_df = pd.DataFrame(feature_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "      <th>cluster</th>\n",
       "      <th>signal</th>\n",
       "      <th>cluster_mean</th>\n",
       "      <th>stock_return</th>\n",
       "      <th>position</th>\n",
       "      <th>local_degree</th>\n",
       "      <th>cluster_density</th>\n",
       "      <th>cluster_size</th>\n",
       "      <th>global_degree</th>\n",
       "      <th>clusters_per_graph</th>\n",
       "      <th>signal_sign</th>\n",
       "      <th>abs_signal</th>\n",
       "      <th>stock_10d_return</th>\n",
       "      <th>cluster_10d_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-09-15</td>\n",
       "      <td>PANW</td>\n",
       "      <td>3</td>\n",
       "      <td>0.169870</td>\n",
       "      <td>0.025216</td>\n",
       "      <td>0.195085</td>\n",
       "      <td>-1</td>\n",
       "      <td>79.898886</td>\n",
       "      <td>0.368781</td>\n",
       "      <td>22</td>\n",
       "      <td>0.172568</td>\n",
       "      <td>0.049569</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.169870</td>\n",
       "      <td>0.195085</td>\n",
       "      <td>0.025216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-09-15</td>\n",
       "      <td>ENPH</td>\n",
       "      <td>12</td>\n",
       "      <td>0.157938</td>\n",
       "      <td>0.025218</td>\n",
       "      <td>0.183156</td>\n",
       "      <td>-1</td>\n",
       "      <td>76.557791</td>\n",
       "      <td>0.301311</td>\n",
       "      <td>31</td>\n",
       "      <td>0.165352</td>\n",
       "      <td>0.049569</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.157938</td>\n",
       "      <td>0.183156</td>\n",
       "      <td>0.025218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-09-15</td>\n",
       "      <td>AXON</td>\n",
       "      <td>7</td>\n",
       "      <td>0.137672</td>\n",
       "      <td>0.024560</td>\n",
       "      <td>0.162232</td>\n",
       "      <td>-1</td>\n",
       "      <td>69.245678</td>\n",
       "      <td>0.293857</td>\n",
       "      <td>20</td>\n",
       "      <td>0.149559</td>\n",
       "      <td>0.049569</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.137672</td>\n",
       "      <td>0.162232</td>\n",
       "      <td>0.024560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-09-15</td>\n",
       "      <td>ULTA</td>\n",
       "      <td>12</td>\n",
       "      <td>0.134709</td>\n",
       "      <td>0.025218</td>\n",
       "      <td>0.159926</td>\n",
       "      <td>-1</td>\n",
       "      <td>86.476733</td>\n",
       "      <td>0.301311</td>\n",
       "      <td>31</td>\n",
       "      <td>0.186775</td>\n",
       "      <td>0.049569</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.134709</td>\n",
       "      <td>0.159926</td>\n",
       "      <td>0.025218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-09-15</td>\n",
       "      <td>AVGO</td>\n",
       "      <td>5</td>\n",
       "      <td>0.117919</td>\n",
       "      <td>0.022253</td>\n",
       "      <td>0.140172</td>\n",
       "      <td>-1</td>\n",
       "      <td>57.730429</td>\n",
       "      <td>0.225714</td>\n",
       "      <td>17</td>\n",
       "      <td>0.124688</td>\n",
       "      <td>0.049569</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.117919</td>\n",
       "      <td>0.140172</td>\n",
       "      <td>0.022253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date stock  cluster    signal  cluster_mean  stock_return  position  \\\n",
       "0 2014-09-15  PANW        3  0.169870      0.025216      0.195085        -1   \n",
       "1 2014-09-15  ENPH       12  0.157938      0.025218      0.183156        -1   \n",
       "2 2014-09-15  AXON        7  0.137672      0.024560      0.162232        -1   \n",
       "3 2014-09-15  ULTA       12  0.134709      0.025218      0.159926        -1   \n",
       "4 2014-09-15  AVGO        5  0.117919      0.022253      0.140172        -1   \n",
       "\n",
       "   local_degree  cluster_density  cluster_size  global_degree  \\\n",
       "0     79.898886         0.368781            22       0.172568   \n",
       "1     76.557791         0.301311            31       0.165352   \n",
       "2     69.245678         0.293857            20       0.149559   \n",
       "3     86.476733         0.301311            31       0.186775   \n",
       "4     57.730429         0.225714            17       0.124688   \n",
       "\n",
       "   clusters_per_graph  signal_sign  abs_signal  stock_10d_return  \\\n",
       "0            0.049569          1.0    0.169870          0.195085   \n",
       "1            0.049569          1.0    0.157938          0.183156   \n",
       "2            0.049569          1.0    0.137672          0.162232   \n",
       "3            0.049569          1.0    0.134709          0.159926   \n",
       "4            0.049569          1.0    0.117919          0.140172   \n",
       "\n",
       "   cluster_10d_return  \n",
       "0            0.025216  \n",
       "1            0.025218  \n",
       "2            0.024560  \n",
       "3            0.025218  \n",
       "4            0.022253  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Label Signals\n",
    "- Label based on profitability \n",
    "- This becomes a target for ML Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idea 1 - Label the signals based on future return threshold\n",
    "\n",
    "labelled_rows = []\n",
    "\n",
    "profit_threshold = 0.045  # % threshold over 5-day holding (4% + .5% for fees)\n",
    "holding_period = 10       # in days\n",
    "\n",
    "for _, row in features_df.iterrows():\n",
    "    date = row['date']\n",
    "    stock = row['stock']\n",
    "    position = row['position']  # -1 for short, 1 for long, 0 for neutral\n",
    "\n",
    "    try:\n",
    "        date_idx = df_returns.index.get_loc(date)\n",
    "        forward_window = df_returns.iloc[date_idx + 1: date_idx + 1 + holding_period]\n",
    "\n",
    "        # Cumulative log return → convert to standard return\n",
    "        log_ret_sum = forward_window[stock].sum()\n",
    "        future_return = np.exp(log_ret_sum) - 1\n",
    "\n",
    "        pnl = future_return * position\n",
    "        label = 1 if (pnl >= profit_threshold) or (pnl >= 0.005) else 0\n",
    "\n",
    "        labelled_rows.append({\n",
    "            **row,\n",
    "            \"log_ret_sum\": log_ret_sum,\n",
    "            \"future_return\": future_return,\n",
    "            \"pnl\": pnl,\n",
    "            \"label\": label\n",
    "        })\n",
    "\n",
    "    except (KeyError, IndexError):\n",
    "        continue  # not enough forward data, skip\n",
    "\n",
    "labelled_df = pd.DataFrame(labelled_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal Ratio (0 to 1):\n",
      "label\n",
      "0    0.90745\n",
      "1    0.09255\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# What is the ratio of the signals (0 to 1)?\n",
    "\n",
    "signal_ratio = labelled_df['label'].value_counts(normalize=True)\n",
    "print(\"Signal Ratio (0 to 1):\")\n",
    "print(signal_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Machine Learning Signal Classifier\n",
    "Train multiple classifiers on the labeled dataset:\n",
    "\n",
    "- Logistic Regression\n",
    "- Gradient Boosted Trees\n",
    "- Neural Net (MLP)\n",
    "- SGD Classifier\n",
    "- AdaBoost\n",
    "\n",
    "Ensemble (soft voting) using classifier probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Features to use\n",
    "feature_cols = [\n",
    "    'signal', 'abs_signal', 'signal_sign',\n",
    "    'local_degree', 'cluster_density', 'cluster_size',\n",
    "    'stock_10d_return', 'cluster_10d_return', 'global_degree', 'clusters_per_graph'\n",
    "]\n",
    "\n",
    "X = labelled_df[feature_cols]\n",
    "y = labelled_df['label']\n",
    "\n",
    "labelled_df = labelled_df.sort_values('date')          # 'date' is a column in the DF\n",
    "\n",
    "# Use the first 150 unique dates as cutoff (10 day holding period) = 1500 days\n",
    "\n",
    "cutoff_date = np.sort(labelled_df['date'].unique())[150]   \n",
    "\n",
    "train_mask = labelled_df['date'] <= cutoff_date\n",
    "test_mask  = ~train_mask\n",
    "\n",
    "X_train = labelled_df.loc[train_mask, feature_cols]\n",
    "y_train = labelled_df.loc[train_mask, 'label']\n",
    "X_test  = labelled_df.loc[test_mask,  feature_cols]\n",
    "y_test  = labelled_df.loc[test_mask,  'label']\n",
    "\n",
    "scaler   = StandardScaler()\n",
    "X_train  = scaler.fit_transform(X_train)\n",
    "X_test   = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogReg...\n",
      "Training HGB...\n",
      "Training MLP...\n",
      "Training SGD...\n",
      "Training AdaBoost...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, VotingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Hyperparameters from grid search in paper\n",
    "\n",
    "logreg_best = LogisticRegression(\n",
    "    C=8,\n",
    "    penalty='l2',\n",
    "    solver='lbfgs',\n",
    "    max_iter=75,\n",
    "    class_weight='balanced',\n",
    "    warm_start=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "SGD_best = SGDClassifier(\n",
    "    loss='modified_huber',\n",
    "    penalty='l2',\n",
    "    alpha=0.001,\n",
    "    max_iter=200,\n",
    "    early_stopping=False,\n",
    "    learning_rate='optimal',\n",
    "    eta0=0.01,  # required even if not strictly used in 'constant' mode\n",
    "    warm_start=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "HGB_best = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.1,\n",
    "    early_stopping='auto',\n",
    "    max_iter=100,\n",
    "    warm_start=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "MLP_best = make_pipeline( \n",
    "    StandardScaler(),\n",
    "    MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 64),\n",
    "        activation='relu',\n",
    "        alpha=0.000001,\n",
    "        learning_rate='constant',\n",
    "        batch_size=200,\n",
    "        solver='adam',\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "ADA_best = AdaBoostClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.001,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"LogReg\": logreg_best,\n",
    "    \"HGB\": HGB_best,\n",
    "    \"MLP\": MLP_best,\n",
    "    \"SGD\": SGD_best,\n",
    "    \"AdaBoost\": ADA_best\n",
    "}\n",
    "\n",
    "# Fit all base models\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating LogReg...\n",
      "\n",
      "Evaluating HGB...\n",
      "\n",
      "Evaluating MLP...\n",
      "\n",
      "Evaluating SGD...\n",
      "\n",
      "Evaluating AdaBoost...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Brier Score</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HGB</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Classifier  Brier Score  Precision\n",
       "1        HGB        0.056      0.518\n",
       "2        MLP        0.069      0.487\n",
       "3        SGD        0.072      0.512\n",
       "4   AdaBoost        0.084      0.000\n",
       "0     LogReg        0.107      0.459"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, brier_score_loss, precision_score\n",
    "import pandas as pd\n",
    "\n",
    "results = {}\n",
    "\n",
    "results = {}\n",
    "summary_rows = []  # For final summary table\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    try:\n",
    "        # For binary classification, get prob for class 1\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        brier = brier_score_loss(y_test, y_proba)\n",
    "    except Exception as e:\n",
    "        print(f\"AUC/Brier error for {name}: {e}\")\n",
    "        auc = None\n",
    "        brier = None\n",
    "        y_proba = None\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    results[name] = {\n",
    "        \"AUC\": auc,\n",
    "        \"Brier Score\": brier,\n",
    "        \"Precision\": precision,\n",
    "        \"Report\": report\n",
    "    }\n",
    "\n",
    "    # For the final summary table\n",
    "    summary_rows.append({\n",
    "        \"Classifier\": name,\n",
    "        \"Brier Score\": round(brier, 3) if brier is not None else None,\n",
    "        \"Precision\": round(precision, 3)\n",
    "    })\n",
    "\n",
    "# Create summary DataFrame\n",
    "results_df = pd.DataFrame(summary_rows).sort_values(by=\"Brier Score\", ascending=True)\n",
    "from IPython.display import display\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Soft Voting Ensemble):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94     48666\n",
      "           1       0.51      0.28      0.36      5617\n",
      "\n",
      "    accuracy                           0.90     54283\n",
      "   macro avg       0.71      0.62      0.65     54283\n",
      "weighted avg       0.88      0.90      0.88     54283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensemble\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(name, model) for name, model in models.items()],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "print(\"Classification Report (Soft Voting Ensemble):\\n\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
