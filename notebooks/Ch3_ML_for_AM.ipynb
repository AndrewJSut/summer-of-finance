{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML for Asset Management - Marcos Lopez Prado \n",
    "\n",
    "**Date:** 2025-05-20 \n",
    "\n",
    "Working through the content in the textbook. Beach day so copying over earlier work.\n",
    "\n",
    "\n",
    "### Chapter 3 - Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Motivation:\n",
    "\n",
    "Infomation theory is really useful for both Finance and ML!\n",
    "\n",
    "The key idea behind entropy is to quantify the amount of uncertainty associated with a random variable\n",
    " \n",
    "In this section, we review concepts that are used throughout ML in a variety of settings, including:\n",
    "1.  defining the objective function in decision tree learning; \n",
    "2.  defining the loss function for classification problems; \n",
    "3.  evaluating the distance between two random variables; \n",
    "4.  comparing clusters; and \n",
    "5.  feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 - Correlation as a Metric:\n",
    "\n",
    "# Correlation is a useful measure of linear codependence, however not a metric (trianble inequality does not hold).\n",
    "\n",
    "# Consider two random vectors X, Y of size T, and a correlation estimate ρ(X,Y) \n",
    "# with the only requirement that σ(X,Y)=ρ(X,Y)σ(X)σ(Y). (covarience)\n",
    "\n",
    "# Then the measure: d_corr(X,Y) = sqrt(1/2(1 - ρ(X,Y))) is a metric, and satisfies the triangle inequality.\n",
    "\n",
    "# We can show the euclidian metric after normalization d(x,y) = sqrt(4T)*d_corr(X,Y)\n",
    "\n",
    "# The implication is that d_corr(X,Y) is a linear multiple of the Euclidean distance between the vectors \n",
    "# X,Y after z-standardization hence it inherits the true-metric properties of the Euclidean distance.\n",
    "\n",
    "# Properties of d(x,y):\n",
    "\n",
    "# 1. Normalized (in 0 to 1)\n",
    "\n",
    "# 2. Another property is that it deems more distant two random variables \n",
    "#   with negative correlation than two random variables with positive correlation, \n",
    "#   regardless of their absolute value\n",
    "\n",
    "#   This property makes sense in many applications. For example, we may wish to build a long-only\n",
    "#   portfolio, where holdings in negative-correlated securities can only offset\n",
    "#   risk, and therefore should be treated as different for diversification purposes.\n",
    "\n",
    "#   ρ = 1 => d = 0\n",
    "#   ρ = -1 => d = 1\n",
    "#   ρ = 0 => d = 1/sqrt(2) = 0.707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hX: 2.8279, hY: 2.9423\n",
      "Mutual Information (I(X;Y)): 0.3334\n",
      "Normalized MI: 0.1179\n",
      "Joint Entropy (H(X,Y)): 5.4368\n",
      "H(X|Y): 2.4945, H(Y|X): 2.6089\n"
     ]
    }
   ],
   "source": [
    "# 3.3-3.7 Marginal and Joint Entropy\n",
    "\n",
    "# The notion of correlation presents three important caveats:\n",
    "#    1. First, it quantifies the linear codependency between two random variables. \n",
    "#       It neglects nonlinear relationships. \n",
    "#    2. Second, correlation is highly influenced by outliers. \n",
    "#    3. Third, its application beyond the multivariate Normal case is questionable. \n",
    "#       We may compute the correlation between any two real variables, \n",
    "#       however that correlation is typically meaningless unless the two variables \n",
    "#       follow a bivariate Normal distribution. \n",
    "# \n",
    "# To overcome these caveats, we need to introduce a few information-theoretic concepts.\n",
    "\n",
    "# Let X be a discrete random variable that takes values in the set Sx with probability mass function p(x).\n",
    "\n",
    "# The entropy of X is defined as:\n",
    "\n",
    "    # H(X) = -Σx∈Sx p(x) log p(x)\n",
    "\n",
    "# The value 1/p(x) measures how \"surprising\" an event is. Entropy is the expected value of the log of those surprises\n",
    "# Accordingly, entropy can be interpreted as the amount of uncertainty associated with X\n",
    "\n",
    "# Reaches a max at log(|Sx|) \n",
    "\n",
    "\n",
    "# The joint entropy of two random variables X and Y is defined as:\n",
    "\n",
    "    # H(X,Y) = -Σx∈Sx Σy∈Sy p(x,y) log p(x,y)\n",
    "\n",
    "# The joint entropy is the amount of uncertainty associated with the pair (X,Y).\n",
    "\n",
    "\n",
    "# The conditional entropy of X given Y is defined as:\n",
    "\n",
    "    # H(X|Y) = -Σy∈Sy p(y) Σx∈Sx p(x|y) log p(x|y)\n",
    "\n",
    "# The conditional entropy is the amount of uncertainty associated with X given that we know Y.\n",
    "\n",
    "\n",
    "# KL-Divergence: \n",
    " \n",
    "# The Kullback-Leibler divergence (or relative entropy) between two probability distributions P and Q is defined as:\n",
    "\n",
    "   # D(P||Q) = Σx∈S p(x) log(p(x)/q(x)) \n",
    "\n",
    "# Intuitively, this expression measures how much p diverges from a reference distribution q (nonnegative but asymmetric).\n",
    "\n",
    "\n",
    "# Cross-Entropy:\n",
    "\n",
    "# The cross-entropy between two probability distributions P and Q is defined as:\n",
    "\n",
    "    # H(P,Q) = -Σx∈S p(x) log q(x)\n",
    "\n",
    "# Cross-entropy can be interpreted as the uncertainty associated with X, \n",
    "# where we evaluate its information content using a wrong distribution q rather than the true distribution p.\n",
    "\n",
    "# Cross entropy is popular in classifcation problems, and it is particularly meaningful in financial applications.\n",
    "\n",
    "\n",
    "# Mutual Information:\n",
    "\n",
    "# The mutual information between two random variables X and Y is defined as:\n",
    "\n",
    "    # I(X;Y) = H(X) + H(Y) - H(X,Y)\n",
    "\n",
    "# The mutual information is a measure of the amount of information that knowing \n",
    "# one of the variables provides about the other.\n",
    "\n",
    "# It is symmetric, nonnegative, and zero if and only if X and Y are independent. BUT not a metric (triangle fails).\n",
    "\n",
    "# I(X;Y) = H(X) + H(Y) - H(X,Y)\n",
    "\n",
    "# I(X;Y,Z) = I(X;Y) + I(X,Y;Z)\n",
    "\n",
    "# Given two arrays x and y of equal size, which are discretized into a regular grid \n",
    "# with a number of partitions (bins) per dimension, the code below shows how to compute\n",
    "# in python the marginal entropies, joint entropy, conditional entropies, and the mutual information.\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "x = np.random.normal(loc=0, scale=1, size=1000)\n",
    "y = 0.5 * x + np.random.normal(loc=0, scale=1, size=1000)  # correlated with x\n",
    "\n",
    "# Set number of bins for histogram estimation\n",
    "bins = 30\n",
    "\n",
    "# Compute 2D histogram (joint distribution)\n",
    "cXY = np.histogram2d(x, y, bins)[0]\n",
    "\n",
    "# Compute marginal entropies\n",
    "hX = ss.entropy(np.histogram(x, bins)[0])\n",
    "hY = ss.entropy(np.histogram(y, bins)[0])\n",
    "\n",
    "# Compute mutual information\n",
    "iXY = mutual_info_score(None, None, contingency=cXY)\n",
    "\n",
    "# Normalize mutual information\n",
    "iXYn = iXY / min(hX, hY)\n",
    "\n",
    "# Compute joint entropy\n",
    "hXY = hX + hY - iXY\n",
    "\n",
    "# Conditional entropies\n",
    "hX_Y = hXY - hY  # H(X|Y)\n",
    "hY_X = hXY - hX  # H(Y|X)\n",
    "\n",
    "# Output the results\n",
    "print(f\"hX: {hX:.4f}, hY: {hY:.4f}\")\n",
    "print(f\"Mutual Information (I(X;Y)): {iXY:.4f}\")\n",
    "print(f\"Normalized MI: {iXYn:.4f}\")\n",
    "print(f\"Joint Entropy (H(X,Y)): {hXY:.4f}\")\n",
    "print(f\"H(X|Y): {hX_Y:.4f}, H(Y|X): {hY_X:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Variation of Information: 5.2269\n",
      "Normalized Variation of Information: 0.9270\n"
     ]
    }
   ],
   "source": [
    "# 3.8 Varition of Information:\n",
    "\n",
    "# This measure can be interpreted as the uncertainty we expect in one variable if we are told the value of other.\n",
    "\n",
    "# The variation of information between two random variables X and Y is defined as:\n",
    "\n",
    "#    VI(X,Y)    = H(X) + H(Y) - 2I(X;Y)\n",
    "#               = H(X|Y) + H(Y|X)\n",
    "#               = H(X,Y) - I(X,Y)\n",
    "#               = 2H(X,Y) - H(X) - H(Y)\n",
    "\n",
    "# The variation of information is symmetric, nonnegative, zero if and only if X and Y are equal,\n",
    "# and has an Upper bound of H(X,Y). It is a metric!\n",
    "\n",
    "# However as H(X,Y) is not bounded except for the size of Sx and Sy, VI is not bounded. \n",
    "# This is problematic when we wish to compare variations of information across different population sizes.\n",
    "\n",
    "# The following quantity is a metric bounded between zero and one for all pairs (X,Y):\n",
    "\n",
    "#    VI_tilda(X,Y) = VI(X,Y) / H(X,Y)\n",
    "\n",
    "# Another way of bounding it is:\n",
    "\n",
    "#    VI_tilda(X,Y) = 1 - I(X;Y) / max{H(X), H(Y)}\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# ---------------------------------------------------\n",
    "def varInfo(x, y, bins, norm=False):\n",
    "    \"\"\"\n",
    "    Compute the variation of information (VI) between two variables x and y.\n",
    "    \n",
    "    Parameters:\n",
    "        x (array-like): First input vector.\n",
    "        y (array-like): Second input vector.\n",
    "        bins (int): Number of bins to use for histograms.\n",
    "        norm (bool): Whether to normalize VI by the joint entropy.\n",
    "    \n",
    "    Returns:\n",
    "        float: Variation of information (normalized if norm=True).\n",
    "    \"\"\"\n",
    "    # Joint histogram\n",
    "    cXY = np.histogram2d(x, y, bins)[0]\n",
    "    \n",
    "    # Mutual information\n",
    "    iXY = mutual_info_score(None, None, contingency=cXY)\n",
    "    \n",
    "    # Marginal entropies\n",
    "    hX = ss.entropy(np.histogram(x, bins)[0])\n",
    "    hY = ss.entropy(np.histogram(y, bins)[0])\n",
    "    \n",
    "    # Variation of information\n",
    "    vXY = hX + hY - 2 * iXY\n",
    "    \n",
    "    if norm:\n",
    "        hXY = hX + hY - iXY  # Joint entropy\n",
    "        vXY /= hXY           # Normalized VI\n",
    "    \n",
    "    return vXY\n",
    "\n",
    "# ------------------ Example Use ------------------\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "x = np.random.normal(0, 1, 1000)\n",
    "y = 0.5 * x + np.random.normal(0, 1, 1000)\n",
    "\n",
    "# Set number of histogram bins\n",
    "bins = 30\n",
    "\n",
    "# Compute VI\n",
    "vi_raw = varInfo(x, y, bins, norm=False)\n",
    "vi_norm = varInfo(x, y, bins, norm=True)\n",
    "\n",
    "# Print results\n",
    "print(f\"Raw Variation of Information: {vi_raw:.4f}\")\n",
    "print(f\"Normalized Variation of Information: {vi_norm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.9 Differential Entropy:\n",
    "\n",
    "# See course notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.10 Distance between two partitions:\n",
    "\n",
    "# A partition P of a data set D is an unordered set of mutually disjoint nonempty subsets.\n",
    "\n",
    "# Let us define the uncertainty associated with P. \n",
    "# First, we set the probability of picking any element d in D to be p(d) = 1/|D|.\n",
    "# Second, the probability that an element d picked at random belongs to subset Dk is p(k) = |Dk|/|D|.\n",
    "\n",
    "# This second probability p(k) is associated with a discrete random variable that takes a \n",
    "# value k from S = {1,2,...,K} with probability mass function p(k) = |Dk|/|D|.\n",
    "\n",
    "# Third, the uncertainty associated with this discrete random variable can be expressed in terms of the entropy\n",
    "#    H(P) = -Σk=1 to K p(k) log p(k)\n",
    "\n",
    "# For another partition P', we can define the uncertainty associated with P' in the same way. \n",
    "\n",
    "# We can define the variation of information as: \n",
    "\n",
    "#    VI(P,P') = H(P|P') + H(P'|P) \n",
    "\n",
    "#  In the context of unsupervised learning, variation of information is useful for comparing outcomes \n",
    "#  from a partitional (non-hierarchical) clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.12 Conclusions:\n",
    "\n",
    "- Correlations are useful at quantifying the linear codependency between random variables.  \n",
    "- This form of codependency accepts various representations as a distance metric.  \n",
    "- However, when variables X and Y are bound by a nonlinear relationship, the above distance metric misjudges the similarity of these variables.  \n",
    "- For nonlinear cases, we have argued that the **normalized variation of information** is a more appropriate distance metric.  \n",
    "- It allows us to answer questions regarding the unique information contributed by a random variable, without having to make functional assumptions.  \n",
    "- Given that many machine learning algorithms do not impose a functional form on the data, it makes sense to use them in conjunction with entropy-based features.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
